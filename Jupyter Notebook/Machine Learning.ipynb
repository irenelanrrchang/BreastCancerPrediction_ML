{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and check the data\n",
    "cancer = pd.read_csv('../Resources/data.csv')\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean           ...             radius_worst  texture_worst  \\\n",
       "0         0.2419           ...                    25.38          17.33   \n",
       "1         0.1812           ...                    24.99          23.41   \n",
       "2         0.2069           ...                    23.57          25.53   \n",
       "3         0.2597           ...                    14.91          26.50   \n",
       "4         0.1809           ...                    22.54          16.67   \n",
       "\n",
       "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0           184.60      2019.0            0.1622             0.6656   \n",
       "1           158.80      1956.0            0.1238             0.1866   \n",
       "2           152.50      1709.0            0.1444             0.4245   \n",
       "3            98.87       567.7            0.2098             0.8663   \n",
       "4           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0           0.7119                0.2654          0.4601   \n",
       "1           0.2416                0.1860          0.2750   \n",
       "2           0.4504                0.2430          0.3613   \n",
       "3           0.6869                0.2575          0.6638   \n",
       "4           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data\n",
    "cancer = cancer.iloc[:,:-1]\n",
    "cancer = cancer.drop(cancer.columns[[0]], axis=1) \n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean           ...             radius_worst  \\\n",
       "0                 0.07871           ...                    25.38   \n",
       "1                 0.05667           ...                    24.99   \n",
       "2                 0.05999           ...                    23.57   \n",
       "3                 0.09744           ...                    14.91   \n",
       "4                 0.05883           ...                    22.54   \n",
       "\n",
       "   texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0          17.33           184.60      2019.0            0.1622   \n",
       "1          23.41           158.80      1956.0            0.1238   \n",
       "2          25.53           152.50      1709.0            0.1444   \n",
       "3          26.50            98.87       567.7            0.2098   \n",
       "4          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ref: 21-2-6\n",
    "# Data Pre Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "target = cancer[\"diagnosis\"]\n",
    "data = cancer.drop(\"diagnosis\", axis=1)\n",
    "feature_names = data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1, Train/Test Score: 1.000/0.930\n",
      "k: 3, Train/Test Score: 0.951/0.930\n",
      "k: 5, Train/Test Score: 0.934/0.965\n",
      "k: 7, Train/Test Score: 0.937/0.958\n",
      "k: 9, Train/Test Score: 0.934/0.958\n",
      "k: 11, Train/Test Score: 0.934/0.979\n",
      "k: 13, Train/Test Score: 0.925/0.972\n",
      "k: 15, Train/Test Score: 0.923/0.965\n",
      "k: 17, Train/Test Score: 0.925/0.965\n",
      "k: 19, Train/Test Score: 0.920/0.965\n",
      "k: 21, Train/Test Score: 0.918/0.965\n",
      "k: 23, Train/Test Score: 0.918/0.965\n",
      "k: 25, Train/Test Score: 0.915/0.951\n",
      "k: 27, Train/Test Score: 0.918/0.951\n",
      "k: 29, Train/Test Score: 0.915/0.951\n",
      "k: 31, Train/Test Score: 0.915/0.951\n",
      "k: 33, Train/Test Score: 0.913/0.944\n",
      "k: 35, Train/Test Score: 0.915/0.944\n",
      "k: 37, Train/Test Score: 0.918/0.944\n",
      "k: 39, Train/Test Score: 0.913/0.951\n",
      "k: 41, Train/Test Score: 0.911/0.951\n",
      "k: 43, Train/Test Score: 0.911/0.944\n",
      "k: 45, Train/Test Score: 0.911/0.951\n",
      "k: 47, Train/Test Score: 0.911/0.951\n",
      "k: 49, Train/Test Score: 0.904/0.944\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VOX1+PHPyQZhDXshQUFBBJVd\nXHDFBdSqKODeulVr1fbbRVus/WqLtdhqW7/91S62RXAvIOKGIsWFuhNA9n2RJCA7yBIISc7vj+dG\nJskkc5PMnZnMnPfrlVdm7tw799wQ5uQ+y3lEVTHGGGNqkxbvAIwxxiQ+SxbGGGMismRhjDEmIksW\nxhhjIrJkYYwxJiJLFsYYYyKyZGGMMSYiSxbGGGMismRhjDEmoox4BxAt7du3127dusU7DGOMaVTm\nzZu3XVU7RNovaZJFt27dyM/Pj3cYxhjTqIjIF372s2YoY4wxEVmyMMYYE5ElC2OMMRFZsjDGGBOR\nJQtjjDERBZYsRGSCiGwVkSU1vC4i8icRWSMii0RkYMhrN4rIau/rxqBiBJi+oIihj7xD97FvMPSR\nd5i+oCjI0xljTKMU5J3FRGBELa9fBPT0vm4H/gogIm2BB4FTgCHAgyLSJogApy8o4r5piynaXYwC\nRbuLuW/aYksYxhhTRWDJQlXnADtr2eVy4Gl1PgFyRKQzMByYpao7VXUXMIvak069PTpzJcWHyypt\nKz5cxqMzVwZxOmOMabTi2WeRCxSEPC/0ttW0vRoRuV1E8kUkf9u2bXUOYNPu4jptN8aYVBXPZCFh\ntmkt26tvVH1SVQer6uAOHSLOVq+mS052nbYbY0yqimeyKAS6hjzPAzbVsj3q7h3ei+zM9ErbsjPT\nuXd4ryBOZ4wxjVY8k8WrwLe9UVGnAntUdTMwE7hQRNp4HdsXetuibuSAXMZfeRJNM92PITcnm/FX\nnsTIAWFbvYwxJmUFVkhQRF4AzgHai0ghboRTJoCq/g2YAVwMrAEOADd7r+0UkYeAud5bjVPV2jrK\nG2TkgFwWF+3h+U838sHPzkUkXCuYMcaktsCShapeG+F1Be6q4bUJwIQg4gonr002xYfL2Lm/hHYt\nmsTqtMYY02jYDG4gr00zAAp32SgoY4wJx5IF7s4CLFkYY0xNLFkAuV8niwNxjsQYYxKTJQugVdNM\nWmdn2p2FMcbUwJKFJ69Ntt1ZGGNMDSxZeFyysDsLY4wJx5KFJ69NMwp3FeNG9BpjjAllycITOtfC\nGGNMZZYsPDbXwhhjambJwmNzLYwxpmaWLDwVyaLARkQZY0w1liw8LZtmktMs04bPGmNMGJYsQtjw\nWWOMCc+SRYi8nGaWLIwxJgxLFiEqZnHbXAtjjKnMkkWIvDbZHDxczg6ba2GMMZVYsghhcy2MMSY8\nSxYh8tpaqXJjjAnHkkWI3BybmGeMMeFYsghhcy2MMSY8SxZV2FwLY4ypzpJFFTbXwhhjqrNkUYXN\ntTDGmOosWVRhcy2MMaY6SxZV2FwLY4ypzpJFFTbXwhhjqrNkUUXFnUXBTruzMMaYCpYsqmjRJIM2\nNtfCGGMqsWQRRl4bGz5rjDGhLFmEUTF81hhjjGPJIoyKWdw218IYYxxLFmHktWnGodJytu+zuRbG\nGAMBJwsRGSEiK0VkjYiMDfP60SIyW0QWich7IpIX8trvRGSpiCwXkT+JiAQZa6i8NjZ81hhjQgWW\nLEQkHXgCuAjoA1wrIn2q7PYY8LSq9gXGAeO9Y08HhgJ9gROBk4Gzg4q1KpuYZ4wxlQV5ZzEEWKOq\n61S1BHgRuLzKPn2A2d7jd0NeV6ApkAU0ATKBLQHGWklumxRY1+KDx2H9nMrb1s9x240xpoogk0Uu\nUBDyvNDbFmohMMp7fAXQUkTaqerHuOSx2fuaqarLq55ARG4XkXwRyd+2bVvUAk+JuRa5A2HKTUcS\nxvo57nnuwHhGZYxJUEEmi3B9DFWHF90DnC0iC3DNTEVAqYj0AHoDebgEM0xEzqr2ZqpPqupgVR3c\noUOHqAaf9HMtup8FYybC5G/Da//jEsWYiW67McZUETFZiEi2iNwnIn/znvcQkYt8vHch0DXkeR6w\nKXQHVd2kqleq6gDgfm/bHtxdxiequk9V9wFvAqf6uqIoSYm5Ft3PgqwWMG8i9DjfEoUxpkZ+7iwm\n4O4SzvCebwJ+4+O4uUBPEekuIlnANcCroTuISHsRqYjhPu9cABtxdxwZIpKJu+uo1gwVpJSYa7Hi\nDdjjtRQumgzzn45vPMaYhOUnWfRU1d8AhwFU9QDhm5gqUdVS4G5gJu6DfrKqLhWRcSJymbfbOcBK\nEVkFdAIe9rZPBdYCi3H9GgtV9TXfVxUFST/XYv0cmHa7ezz6Kchu45qjlkyLb1zGmISU4WOfEhFp\nitffICLdAV+foKo6A5hRZdsDIY+n4hJD1ePKgO/6OUdQQudadGjZJJ6hBKNoPuQNhi8XQ5/LoX1P\n+OcF8OZY6HEeNG0d7wiNMQnEz53FOOAtIE9EJuFGKd0XaFQJIOnnWpz+fdi8CHpcAGnp8I2T4NoX\noHgH/PsGKE3SOypjTL3Umiy8WdMLgTHAbcDLwBBVnV3bcckgL9nnWhTmQ/FOOG74kW3HnguXP+Ga\nqF65C5K5v8YYUye1NkOpqorI66o6CHglRjElhOZNMmjbPIuCZB0RtXomSDocO6zy9n7XwJ5CeOch\naJ0H5z8Yn/iMMQnFT5/FZyIyUFXnBx5NgqkYEZWUVs2Eo06D7Jzqr535E5cwPvgDtM6Fk78T+/iM\nMQnFT5/FGbiEsVJE5ovIAhFJicSRtHMt9hTCliWVm6BCicDFj8FxI2DGvbBiRvj9jDEpw8+dxcjA\no0hQeW2aMXv5VlSVGBa9Dd6qme57TckCID0DRk+Aid+EqbfATa+70VPGmJQU8c5CVdcC2cAF3ldT\nb1vSy2uTzaHScrbtOxTvUKJr9duQczS0P672/bKaw3X/hpad4PmrYEdK/LMbY8LwU+7jbmAycJT3\nNVlE7gw6sESQlCOiDhfDuvddE5Ofu6UWHeH6l9zIqOdGw/7twcdojEk4fvosbscNl/25qv4cOAW4\nI9iwEkNSzrVY/18oLYbjLvR/TPse7g5j90Z46iIoCenHsbLmxqQEP8lC8Ep9eA7jo9xHMsjNScIV\n81bPhMzmcPQZkfcN1XUInPVT2L4KnhkJ5WVW1tyYFOKng/sZ4BMRecl7fgUwKbiQEkfFXIukubNQ\nhVVvwzHnQGbTuh9/zs/cRL5P/wb/GOaKEFpZc2NSgp8O7t/hmqIOAMXAHar6WNCBJYqkmmuxdTns\n2Vi3JqiqLvot5A2BzZ9DxxMsURiTIvx0cJ8MLFfVP6jq74EVIpIyYyiTaq7Fam/IbM8GJIv1c2Dn\nWujQGzbMgf/8KjqxGWMSmp8+iydxdxUV9gN/DyacxJPXphlFybKuxaq34Rt9oVWX+h1f0UcxZiJ8\n933odJKb5f3Rn6IZpTEmAflJFmmqWl7xxHucGVxIiSVp5loc2AkFn9Q+ES+SovlH+igymriJeq2P\ngtm/hi+XRC1UY0zi8ZMs1ovI90QkXUTSROQuYEPAcSWMpJlrsfYd0HI3v6K+zvhh5T6K7By45U1o\n1g6eG+PKiBhjkpKfZPFd4DxgC7AVt8TpbUEGlUiSZq7FqpnQrD10ifIw19Z5cMNUKNkHz46G4t3R\nfX9jTELwMxpqi6qOVtX23tdVqrolFsElgtAV8xqt8jJYMwt6XgBpfv4+qKNOJ8DVz8KONd7CSY28\nyc4YU02NnxwicouI9PAei4g8KSI7vMqz/WMXYnw1y8qgXfMsCnY24juLwrlQvKth/RWRHHM2jPwL\nbPgvTL8TyssjH2OMaTRq+zPzx8AX3uOrgZOBPsDPgZQa/tLoh8+umglpGdUXOoq2vlfBeQ/Ckqkw\n24bUGpNMaksWpapaUebjUmCS1yT1FtAi+NASR8Xw2UarYqGjpq2DP9cZP4LBt8KHj8Nn/wj+fMaY\nmKgtWaiIdBKRJrgO7v+EvJYdbFiJJa9NNoW7iykvb4RzLXYXwNalwTZBhRKBix+FXhe7hZOWvx6b\n8xpjAlVbsvglMB9YB7ypqksARORMYH3woSWOvDbZlJSWs70+cy0+eNxNZgsVy0qtX8/ajlGyAEhL\nh1H/cpP/ptwEBZ8deS3Stdfn51XXY2JxDmOSTI3JQlVfAboD/VX15pCXPgeuCTqwRFIxfLagPk1R\nuQPdB2bFB02sK7WuehvadIP2PWNzvgpZzVwdKS2DZ66E7Wv8XXt9fl51PSYW5zAmyUhSlLEABg8e\nrPn5+YG89+ote7ngj3P4v2v6c3n/3Lq/wfo58MK10PUUV4AvVpVaSw7A77rDoJvcB3c8LHwRXr7D\nrbpXVgJdBrhJfLU5sAM2LYCco9waGkEc05Bz5A6G7Sut4q5JCiIyT1Uj1vvzU6I85eU2dBZ397Mg\nsxmsnQ3Hnh+7D5gN/4XSgw0rHNhQ/a6BDR/AgmegeUc4fAD2+BhZ1jTHzdsI8pj6nCMzGzZ+BANv\ntERhUoolCx8q5lrUO1msfQf2bwXJgLX/gfd+69aGCNoqb6GjbnVc6Cia1s+BlTPcwkn5/4Lhv4n8\nIVvRxBPkMfU9x+Qb3TDkhS/CSaMtYZiU4adE+YsiMlzEz4LNyavecy3Wz4EpXpfPJY9B+17w3m/g\n04AL96rC6rfh2HNd0b94CK1SO+x+9z203T9exzTkHFdNgkE3u76Yyd+u/Rhjkoif2g8TgVuAVSLy\n64pZ3amm3nMtiubDEK+UVt5guPlNaNkFZj0I21dHN8hQW5e5lezi2QQVWqUW3PcxE932eB7T0HOc\n+j1XQqXnhbUfY0wS8d3BLSJtgOuBn+GGzv4DeEFVS4MLz78gO7gBxs9YzlMfbWDFuBGkpdXxJmv2\nODfE8v7N7q/8nevgnxe4Tt/v/AdadIx+wP/9g5tF/eMV0Kpz9N8/1T1/DRR+Bj9a6voxjGmk/HZw\n+6oq5yWK64BvAYtwix+dDrzVkCAbkwbNtdiyzA1drWgOansMXD8Z9m9zpb0P7YtusOCaoDr3s0QR\nlNPudKOjFk2OdyTGxISfPovJwEdAW2CUql6iqs+p6veACGMNk0eD5lpsXQod+1TeljsIRj8FXy6C\nqTdDWRRv0A7shIJPYzsRL9V0O9OtFPjJX13/kDFJzs+dxT+BPqr6kKpWWt1GVQfUdqCIjBCRlSKy\nRkTGhnn9aBGZLSKLROQ9EckLee0oEXlbRJaLyDIR6ebvkoJR71Llh/a6cfyd+lR/rdcIuOT37i7g\njR9H70NnzeyGL3Rkaifi7i62LXej3YxJcn6SxTHA1xXoRKSNiNwe6SARSQeeAC7CVau9VkSqfmI+\nBjytqn2BccD4kNeeBh5V1d7AENzCS3FT70WQti533zudGP71wbfAmffA/Ekw57EGRBhi9Uxo3sFN\nNDPBOXGUm6Px8RPxjsSYwPlJFneo6tfLn6nqLuB7Po4bAqxR1XWqWgK8CFxeZZ8+wGzv8bsVr3tJ\nJUNVZ3nn3Keqca0Rnp2VTvsW9ZhrscVbm7pqM1SoYb+AftfCu7+GBc/VP0hwzVmrZ0GPgBY6Mkdk\nNHEj3dbOhq0r4h2NMYHy82mSHvpERNKATB/H5QIFIc8LvW2hFgKjvMdXAC1FpB1wHLBbRKaJyAIR\nedS7U6lERG4XkXwRyd+2bZuPkBomt02zujdDbVkGWS1dWYmaiMClf4JjzoHXfuCakeqrcC4c3B27\nKrOpbvAtkN4EPvlLvCMxJlB+ksUsEXlBRM4WkbOA56hcrrwm4caXVm2Uvwc4W0QW4Nb2LgJKcTPL\nz/RePxnXFHZTtTdTfVJVB6vq4A4dOvgIqWHcxLy6NkMtg469XUKoTUYWXPUMdDgeXrgG8idWft1v\nFdXV3kJHGU2sImosNG8P/a6GRf+G/TviHY0xgfGTLO7FjYb6EfAT4APch3gkhUDXkOd5wKbQHVR1\nk6pe6XWU3+9t2+Mdu8BrwioFpgNxL++Z1yabol11WNdCFbYsDd+5HU7TVnD9FGjSCl7/ISye6rbX\npYrqqpnQoTe8cpdVRI2VU+90NbjyJ8Q7EmMCE7E2lKqWAf/P+6qLuUBPEemOu2O4BjdX42si0h7Y\nqarlwH3AhJBj24hIB1XdBgwDgptx51Nem2aUlJWzbd8hOrVqGvmAvZtdk1BNndvhtOoCN70O/zgP\npt0Oy16Bte8eGde/9OXwx538HVfZtmSfK1p43b+tblGsdOztlqyd+w8Y+oP4lVcxJkARk4WIHAs8\njOuM/voTUlWPq+04VS0VkbuBmbh+jwmqulRExgH5qvoqcA4wXkQUmAPc5R1bJiL3ALO9mlTzcDPG\n4yp0+KyvZLFlqfteW+d2OB17u0l7ky6D5a+6be/XocR4/+stUcTaqXfBc6NgyTTof228ozEm6vxU\nnZ0I/Bo3zPUi4Gag3M+bq+oMYEaVbQ+EPJ4KTK3h2FlAXz/niZWuIaXKBx3t44CKZOG3GSqUlrs1\ns08Y6T6Ahj/sJvLVpGgezLzfjapaPBn6XGYJI5Z6nOeKRH7yhCvLntp1N00S8tNn0UxVZwKo6lpV\n/QVwbrBhJabcnDrOtdi6zBUNzG5TtxOFVjj95h/h6mdg1gOuPEjH3tW/9m9zr1/9DFz0iL8qqia6\nKibpfbnYrd9hTJLxkywOeU1Ba0XkDhG5FAig8l3iOzLXwufw2S3L6ndXEYsqqib6+l7tVtuzYbQm\nCflphvoR0AL4Aa7vohWuZHlKcnMtfNxZlB12S2/2GFb3k5zxw+rbup9Vc7NSXfc3wcjMdvMu5jwG\nO9ZCu2PjHZExUVPrnYU3Ee4KVd2rqhtV9Vuqermqfhij+BKO77kWO9a4Nac7nhB8UCZxnPwdN8/l\n07/FOxJjoqrWZOENmx0So1gaBd9zLRrSuW0ar5bfcMutLngOindH3t+YRsJPn8V8r+zGtSJyWcVX\n4JElqNC5FrXaugwkHdrXOsLYJKNT74TD+11xSGOShJ9k0QnYD1wMjPG+RgcZVCLzXaq86oJHJnV0\n7uvWu/j0yeiuU2JMHPmZwf2tWATSWPiea7F1KeSdHJugTOI59U548VpY/oorZW5MI+dnBveT4bar\nasQ1LZKRr3UtDn7lFjwaeGOMojIJ57gRbvncj/9iycIkBT/NULNDvj7EzbGox0LUyaFpZjrtWzSp\nvRnq6wWPbCRUykpLg1O+B0X5UPBZvKMxpsEiJgtV/XfI1yTgSlydqJQ0fUERe4pLeOGzAoY+8g7T\nFxRV32lrPWtCmeRycDdkNq+8kl5tpeahcrl5P8fUdf/6HpOoYvHzSqa4GqA+S6l1B/xURko60xcU\ncd+0xRwuc8Nmi3YXc9+0xdUThp8Fj0zyO+pU0DJXOXj3xsil5qFyuXmoW3l6P/vX95hEFYufVzLF\n1QCiWvt8ARHZxZFFi9KAncBYVZ0ccGx1MnjwYM3PD7aK+dBH3qFod/W+itycbD4cGzJT+6mLobwU\nbn070HhMI7DkJZh6C24tMIW0TJAIf6NpOZQfdkOvtSzyMXXdP/SYzv1gT2HlcjGNTf5EeONHgNTt\n59U0xz2/+plgrn3Bc/Dq3XWPq0krt19QcVUhIvNUdXCk/fyU+2gf8rhcI2WXJLYpTKKotl3Vrbt9\nwpUxisoktBNHwZKXYcVrkHcKHH2av+O++BgKP/V/TF33B7eq4uaF0PWUxpsodm+E98a75r6SvXX4\neX0EhZ+5hBFEc/HeL+G9RyCjKRw+UPd/x6yW0K5H9ONqAD/J4hLgfW8FO0QkBzhDVV8PNLIE1CUn\nO+ydRZec7CNPvtoEB/dY57Zx1s+BjR/BWT+F/H/Beb+I/MG8fg4seMb/MXXdP/SYzv2h4FN482dw\nUR3WTEkExbvg2dFwaC+kZ9b959X/evj8OXjqIrj9fchqFp24Du2F58bAvq2Q2QROq2NcA290Ezqf\nugi++1+3gmYC8NNnMa4iUQCo6m7goeBCSlz3Du9FdmZ6pW3ZmWncO7zXkQ1bl7nv1rltKtqdx0yE\nYff7Kx1f12Maeo7vzIa8Ia6W1fu/q+sVxs/hg/Di9bBzLaSluyabuv68Rv4Fzr0ftq+Cpy+PzgTK\nssMw+Ub4cglkZMHVz9Y9rsv+BOf/CnZtgEnfhNKShscVBX6SRbh9/NyRJJ2RA3IZf+VJ5IbcSXzr\ntKMZOSD3yE5WE8pUqE/p+FiUpw89Jj0Dvj3dlaV5/7dQMLcOFxgn5eUw/Q744kPoMxKuea7+P6+z\nfwqn3OGapN78qWtGri9VeO2HsHY29L60YXGd8UM4/X9cM+Gr329YXFHip4N7IrAVeALX0f19oJOq\nfjvw6OogFh3coUpKyznlN/9haI/2/Pm6kBEL0253i9/8eFnMYjGmwfZvh39d4JpQb52V2OXVZ94P\nH/8ZLnjIrXkeDbMehA8fh/MehDN/XL/3eHc8vP8InD0Wzr0vOnG9/zt492E48x4473+j855V+O3g\n9nNncbe33yvAq7iEcWfDwmv8sjLSuLx/Lm8v28KeA4ePvLBlqTVBmcaneXu43lvh+NlRsG9bfOOp\nySd/dYliyHfh9O9H733PexBOGgOzfwUL/1334+c/7RJF/xvgnLHRi+use2Hgt+G/j0H+hOi9bz34\nmZS3T1XvUdX+3tdPVXVfLIJLdKMH5VFSWs5riza5DWWHYdtKa4IyjVO7Y+G6yW4kz/NXQcn+eEdU\n2bJX4K374Phvwojx0V3nPC0NLn/CFYB85S5Y957/Y1fPcs1Px54Hlz4e3bhE4JI/Qs8L4Y2fwMq3\novfedRQxWYjIW94IqIrnbUTkjWDDahxO6NKK47/RkqnzCt2GHWvcOGlb8Mg0VnmDYfQE2Py5mx+S\nKFVzN34CL93minOO+qfr1I62jCauQ7p9T3jxBreeeiSbFrgO7U4nwFWT3KisaEvPgNFPuTkxU2+G\nwnnRP4cPvkqUeyOgAFDVXUCX4EJqPESE0YPy+LxgN2u27g3p3LZkYRqx4y+Gix+DVW/BjHvi37m6\nbRW8cA3kdIVrX3TL1wYlOweunwJNWrrhr3sKa9531wZ47iq37nrFMUFp0sLd9TXv4O76dq4L7lw1\n8JMsykUkr+KJiFgNixAjB+SSkSZMmVfohs2mZdiCR6bxO/lWOOPHMO8p+OAP8Ytj7xZ4bpT7f3X9\nVGjeLvhzts6DG6a6ZrhnR4df8fDATvdaWYnbt+U3go+rRUe4YZqb6f3sKDcoIYb8JIsHgA9F5CkR\neQqYA/w82LAaj/YtmnBOr468PL+I8i+XQLuebny1MY3deQ9A36th9jhY+GLsz39oHzw/xn0oXjcZ\n2naP3bk7neCapHasgX/fAKUhhbYPF7s7nd0b3Z1Oh141v0+0te/hzvnVJhdDSYRF2KLITwf3G7h1\nuCtGQw1R1TeDDqwxGT0oj617D3Fo0xLr3DbJQwQu+zN0Pxumfw8+/FPl14Oshlt2GKbcCJsXu9I5\n8Siod8zZbuLehv+6v+TLy6G8DKbd5ma997ncf2mVaDrqFNdvUzgXnhnpYqoQYKVav1VnDwIbgS1A\nDxE5PZBoGqlhx3ckL/sw2fuLbNisSS4ZWW52dOujYNYDMM9bVzzIarjr3ofXfwRr/gNZ2dDv6uhd\nT131vQoGfNsljJdugZk/h+WvuVpUA+O4iGjvS93w4YJP3Z2PauCVav1MyrsF+AmQCywGTgY+UdVz\nAomonmI9Ka+qfz3/Ireu+i77rnyOFn2/Gbc4jAnEniL4+1lwYAd84yTYvtLdcdTWVr/3S1j/PrTv\n5W//imPWvevuLDKzXfNTvIscqsKL18HKGe55RlPXoR3vuMAlh6Uvu5/tliX1qh4czaqzPwIGAx+r\n6pkicgLwizpFkwJGdNwBq+Dt7W2xerMm6bTOhRtfg0mXwZeLIKuFG1oaaXhpWmbd9gdIb+KSxWl3\nJ8YHsojrv3jybBf/ad9PjLgARk1wK3Ouf9/N8g4wLj/J4qCqFosIIpKlqktF5PjAImqkcg+tYz/N\nmLS0jCuHRd7fmEbnwHag/Eh11yufjFxFdcpNcOqd/vYPd0z3sxLjg/mLD12ncsW1H5MocX0A+7fB\nmT9xI9eOOTuwuPz0WWz2JuW9BswUkZdwfRcm1JZl7Gvdk4VFX7Fqy954R2NMdMW6Gq7fY2KhMcR1\n3gOBx+VnNNRlqrpbVf8X+DXwHHB5INE0VqqwdSmtuvUjI014aV4tE3mMaYxiXQ3X7zGxYHEBPjq4\nG4u4dnDvKYI/9oGLH+O2FQP4vGA3H48dRkZ6fZY4N8aY2Ilm1dmGBDFCRFaKyBoRqVaKUUSOFpHZ\nIrJIRN4LnSnuvd5KRIpE5M9BxtlgFQsedTqB0YPy2Lb3EHNWJ2jVTmOMqYfAkoWIpOPWwLgI6ANc\nKyJVJyE8Bjytqn2BccD4Kq8/BLwfVIxRs2WJ+96xN8OO70i75llHigsaY0wSCPLOYgiwRlXXqWoJ\n8CLV+zr6ALO9x++Gvi4ig4BOwNsBxhgdW5ZBq1zIbkNmulvn4j/LtrJrf2Ish2iMMQ3lp0T5LhHZ\nWeVrvYhMEZFutRyaCxSEPC/0toVaCIzyHl8BtBSRdiKSBvweuDdCbLeLSL6I5G/bFsdmn63LKs3c\nHj0oj5Kycl5duCl+MRljTBT5ubP4f8D/AscCPXAT8iYC04Gnajku3AogVXvT7wHOFpEFwNlAEVCK\nW4lvhqoWUAtVfVJVB6vq4A4dOvi4lAB8veDRkbLkfbq0ok/nVtYUZYxJGn4m5V2oqqeGPP+LiHyi\nqqeKyE9rOa4Q6BryPA+o9Ke2qm4CN+FZRFoAo1R1j4icBpwpIncCLYAsEdmnqlFcrzBKKhY8qrKG\nxehBeYx7fRkrvvyK47/RKk7BGWNMdPjqsxCRK6s8rrhrKK/lsLlATxHpLiJZwDW4qrWh79vea3IC\nuA+YAKCq16vqUaraDXf38XRCJgo4suBRlQKCl/fvYnMujDFJw0+yuAG4zeur2AHcBnxLRJoBP6zp\nIFUtBe4GZgLLgcleqZBxInKZt9s5wEoRWYXrzH64/pcSJ1uWhl3wqF2LJpzXuyMvL9jE4bLacmp4\n0xcUMfSRd+g+9g2GPvIO0xfpspS+AAAZWUlEQVQURStiY4yps4jNUKq6Bjf8NZxah7Wq6gxgRpVt\nD4Q8ngpMjfAeE3F9JIlp67IaFzwaPagrM5du4f2V2zi/Tyffbzl9QRH3TVtM8WFXp75odzH3TXMF\n2EYOqDpGwBhjgudnNFR7EfmpiPxFRJ6s+IpFcI3ClmU1rrl9Tq8O9Zpz8ejMlV8nigrFh8t4dObK\neodpjDEN4acZ6hVcE9EHuDkRFV/m4B7Ys7HG1fEy09MYOSCX2Su2sNPnnItVW/ZStLs47Gubathu\njDFB8zMaqrmq/iTwSBqjrcvd947h7yzAjYr61wfrefXzIm4aGn4N4T3Fh3lt4SamzCtkYUGYxeE9\nIvDLV5cyZnAeJ3Rp3aDQjTGmLvwkizdF5EJVTfyZ1LFWMRKqlnW3e3duRV5OUx6esZxfvbaMLjnZ\n3Du8F5f168JHa3cwZV4Bby35kkOl5fTq1JJfXNKbJplp/OaNFZWaorLS0+jTpSXPf7qRiR9toE/n\nVowZnMfI/rm0aZ7F9AVFPDpzJZt2F399jkj9G/U5xhiTmvwsq7oLaA0cAEpww2ZVVdsGH55/cak6\n+8ZPYNFkGLvR/dkfxvQFRdw7dSGHy478nDPShBZNMthdfJhWTTO4vH8uYwbncVJua8R7n5o+yHcf\nKOGVzzcxZV4BS4q+Iis9jd6dW7B88z5KQkZdZWemM/7Kk2r88K/aie7nGGNM8vFbddZPskgPt11V\ny8Jtj5e4JIsJF4GWw60za9xl6CPvhO2DaJKRxqNj+nFhn040zQz7I45o2aavmDKvgEkfbaA8zD9j\nyyYZ3DS0W9hjJ364gb2HSqttz83J5sOxttSfMamiwWtwi0hPVV0N1NQgv6i+wSUFb8EjThxV6241\ndUqXlJZzWb8uDQqhT5dWPNjlBCZ+uCHs63sPlfLEu2vCvhYuuYB1ohtjwqutz2IscCuuzHhVCiTA\nArRx9FWRGw3Vseb+CoAuOdlh7yy65GRHLZSazlHbXUJNdzzRjMsYkzxqHDqrqrd6D4ep6pmhX8B5\nsQkvgW05suBRbe4d3ovsKs1M2Znp3Du8V9RCqc85wh2TkSZRjcsYkzz8jIb6FBjoY1tq2VpRE6p3\nrbtVdBYHOeqoPueoekyTzDQOl5ZzUp4NyTXGVFdjB7eIdAQ64xYtuoojxQNbAf9U1eNjEqFPMe/g\nfuk2+OIj+PHS2J0zQFv3HuSCP8yhR8cWTP7uaaSnhR/dZYxJLtFYg/sS4M+40uJPhHz9HLe+RWrb\nuqzW+RWNTceWTXnw0j7M+2IXkz7aEO9wjDEJprY+i6e8/olbVfWskD6Li1V1SgxjDM4Hj8P6OZW3\nrZ/jtte2f8WCRx371L5/I3PFgFzO7dWB381cwRc79sc7HGNMAvFTG6qjiLQCEJG/ichnIpIcHdy5\nA2HKTUcSxvo57nluDd0xFfsvfNEteJSWXvv+jYyI8JsrTyIzLY2xLy2mvKbxtcaYlONnUt4iVe0r\nIhcCPwAeBJ5U1UGxCNCvevdZLH8DJt8AWS2gZB80awcZTWvev/Qg7N8OKDTNgaufge7JNYr4xc82\nMnbaYn498kRuOPXoeIdjjAlQgyflhajIJhcBT6nqvJDV7Rq/7me65qQtS6DTidC5X+RjNi90+598\nW9IlCoCrT+7K64s2M37Gcs49viO5NvfCmJTnJ1ksFJEZwHHA/d5a2cnTPrH5c9i7Gc76KeT/C0aM\nrz0BrJ8Dq946sv8xZyVdwhARxl95EsMfn8PYlxbx9C1Dvq5ZZYxJTX7uEG4GfgkMUdUDQFPczO7G\nr6KPYsxEGHa/+x7ah9HQ/Ruxrm2bMfai4/nv6u1MsXXEjUl5EZOFVzDwGOB73qZsP8c1CkXz3Qd+\nxZ1B97Pc86L50dm/kbvhlKMZ0r0tD72+jC1fHYx3OMaYOPLTwf1nIBM4S1V7i0hbYKaqnhyLAP2K\nS9XZFLBh+35G/N8czujRnn98e7A1RxmTZKIxKa/C6ar6XeAggKruBLIaGJ9pJLq1b849F/biP8u3\n8urCTfEOxxgTJ36SxWFv9JMCiEg7oLz2Q0wyuXlodwYclcODry5l295D8Q7HGBMHNSYLEakYKfUE\n8BLQQUR+BXwA/DYGsZkEkZ4mPDq6LwcOlfHgq0viHY4xJg5qGzr7GTBQVZ8WkXnA+bhigmNU1T4x\nUkyPji35n/N78ujMlQwc9za7DhwObN1uWxvcmMRTW7L4uidTVZcCyVFe1dTbN1o1QQR2HjgMQNHu\nYu6bthggah/mVdcGD+Icxpi6qy1ZdBCRH9f0oqr+IYB4TAL7w6zVVB08V3y4jEdnrojKB3lZufLQ\n68u+ThSVz7HSkoUxcVRbskgHWhByh2FSW03rcxftPsgT767hyoG5dG5d99Ig67btY+q8QqbNL2LH\n/pI6ndsYExu1JYvNqjouZpGYhFfTWt9Z6Wk8OnMlv397JWf07MBVg/M4v3cnmmam19j/sO9QKTMW\nbWZyfgH5X+wiTeCcXh0pKS37upkrVHqasH77frq3bx6LSzXGVFHbSnkLVHVAjOOpN5uUF7yq/Qng\n1voef+VJDDgqh6nzCnlpXiGb9hykdXYmJ+W2Yu6GXRwqPTLSOisjjX65rVm6+SsOlJRxTIfmjBnU\nlSsH5tKpVdOw58hKTyM9DdLT0vjtqL5c0rdzTK/bmGTmd1JebcmirTcBr1GwZBEbkUYqlZUrH63d\nzpT8whon8Qmusu2YwV0ZeFROtVnh4c5xcve23P38fBZs3M1Np3fj5xf3JisjOarOGBNPDU4WjY0l\ni8TTfewbYcsTC7D+kUvq/H4lpeU88uYKJny4nn5dc3jiugHktWnW4DiNSWXRLPdhTL10qWEdjJq2\nR5KVkcYDl/bhbzcMZN3WfVzypw94Z8WWhoRojPEp0GQhIiNEZKWIrBGRsWFeP1pEZovIIhF5T0Ty\nvO39ReRjEVnqvXZ1kHGaYNw7vBfZmemVtmVnpnPv8F4Net8RJ3bmte+fQW5ONrdMzOe3b61g2rwC\nhj7yDt3HvsHQR95h+oKiBp3DGFNZYM1QIpIOrAIuAAqBucC1qrosZJ8pwOuqOklEhgE3q+q3ROQ4\nQFV1tYh0AeYBvVV1d03ns2aoxBTkbOyDh8v41WvLeOGzjaQJhC4ZXtHxbnMzjKld3PssROQ04Jeq\nOtx7fh+Aqo4P2WcpMFxVC8X1cu5R1VZh3mshMFpVV9d0PksWqWuAV36kqtycbD4cOywOERnTeCRC\nn0UuUBDyvNDbFmohMMp7fAXQ0qtq+zURGYIrib42oDhNI7c7TKIAN5EvWQZwGBNvQSaLcDO/q/7P\nvQc4W0QWAGcDRUDp128g0hl4Btc8Va0suojcLiL5IpK/bdu26EVuGpWaOswVGPb793ni3TV8ucdW\n+jOmIeLaDFVl/xbAClWt6ORuBbwHjFfVKZHOZ81QqSvcRL6mmWlcMSCXtdv289n6naQJnHVcB8YM\n6sr5fTry5uIv69yXEqtquKlcdTeVrz1e/DZD1Vbuo6HmAj1FpDvujuEa4LrQHUSkPbDTu2u4D5jg\nbc8CXgae9pMoTGqr+DCp6UNmw/b9bnb5/ELuen4+2ZlplJQpZV6PuJ/KtrGqhpvKVXdT+dobg0An\n5YnIxcDjuKKEE1T1YREZB+Sr6qsiMhoYj2sxmAPcpaqHROQG4Ckql0W/SVU/r+lcdmdhIikrVz5Y\ns507nsmn+HD1xR6z0tPo17V12GMXFuyhpKz6MdHuRD9t/Gw2h2kyS4XO+tMfmc2m3al57fGUCHcW\nqOoMYEaVbQ+EPJ4KTA1z3LPAs0HGZlJPeppw9nEdOBgmUQCUlJWTmR6+Gy9cogD31++qLXs5rlPL\nesdVVq7MWb2NqfmFYRNFxXkO1xJfY7ZxxwGmzisImyjAKg4nikCThTGJqKbqubk52Tx/26lhjxn6\nyDthjwG48I9z6JfXmtGDu3JZvy60zs70Fcf67fuZkl/AtPlFfPnVQdo0y6R5k3T2HyoLu/9p42dz\nxYBcxgzu2qDklAgOlJQyY/GXTMkv4NP1OxGBJhlplYpOVmjXIisOEZqqrDaUSTm1Vc/122dRccz9\nlxzPoVJlSn4BK77cS5OMNIaf8A3GDM5j6LHteXXhpkp9Kd8f1oM0EabMK2DuhiOl2ccMymNYb9fx\nHq6z/oZTjmbjzgO8s2IrpeVKv645jBmUx6X9uvDuiq0J2Vlf9Rz3XHgcXds2Y0p+Ia8v2sT+kjK6\ntWvGmMGu6vCn63ZWu3bBtVH/+ILjuPvcHqSl2fI60Rb3SXmxZsnC1EV9PixrO0ZVWVL0FVPmFTB9\nQRFfHSwlJzuDfYfKKC2v/n+saml2v+fZvu8Q0xcUMSW/kJVb9pIuAEJZyP/j+ia+aM54D3eOig/+\nZlnpXHJSZ8YM7srJ3dpUqjpc9dp/MKwHH6/bwfTPN3Fmz/Y8fnV/2rVoEpUYjWPJwpg4OXi4jFnL\ntnDPlIVhm1Xat8hi7v3nVyvNXheqyuKiPVz75CfsL6nebCVAiybhW5n3HSoNWw04mh3Jp4+fzaYw\n/S85zTL58GfDaF5DbOGoKi98VsAvX1tK22ZZPHH9AAYd3TYqcZoE6eA2JhU1zUzn0n5d+MELC8K+\nvmNfSYMSBYCI0DcvhwNhEgW4v+DHDO4a9rUJH64Pu71odzGfrtvBkO5t6x3f0k17mJJfGDZRAOw5\ncLhOiQLctV53ylH0zWvNXc/P5+q/f8LPRhzPd87s3uCfo/HPkoUxAampI72+Jdrrco7cnGweuLRP\n2GNmLv0y7DECXP3kJxzdrhmjB+YxalCer1h37S9h+ueuaWzZ5q/ISk8jOzMt7PDkhlz7ibmtee37\nZ3DvlIU8PGM5n23YyWNj+vkeUGAaxpqhjAlIvPoG6ttn8avLTiAjXZiSX8jH63YgAmf0aM+YwV25\nsE8n3lpyZNZ755ymfLNvZwp3FfOfZVspKSvnxNxWXOWNCHtv5bbArl1VmfDhBsbPWE7nnKZcNagr\nL84tsFnf9WR9FsYkgHiMOorGaKiCnQeY4q2pXrS7mKYZwuFyvp71XqF5VjpXn3wUYwbn0btzqzqd\no6HmfbGLWyZ+xp7i0krbrTx93ViyMMY0WHm58vG6HXxnUn6lu4QKXVo35aP7zotDZE4qz3iPlkQo\nUW6MaeTS0oShPdpzMEyiAGqccR4rNVUTLtpdzFtLNlMSZjSaqR/r4DbGRBSLzvr6qCmuNIE7np1P\n2+ZZjOyfW6mZLIhmu2gck+gVdy1ZGGMiund4r7Ad1g1dT72haorr4ZEn0KZ5E6bMK+CZTzYw4cP1\nnJTbmuM6teCNxZu/rg8WVMXhuh7TGCruWrIwxkQUqQx8osZ17vEd2bW/hFc+L2JyfiEvzS+q9h7F\nh8t46PVltGwa/uPwodeXVeuvifYxNe3/6MyVcf8ZV7AObmNMyug+9o2ws9cTlQDrH7kk2HPYDG5j\njKmspj6ODi2b8K8bw39e3jopn217DwV6TE37x7tPKJQlC2NMyqipj+P+i3vTNy8n7DH3X9w78GPC\n7Q9wzZDwJVviwZKFMSZl1KfvJRbHVN2/U6umHDxcxtMff8EVA3LJa9OsXtcbTdZnYYwxCWjVlr2M\n+utHdGrVlJfuOJ3WzYKpgWWT8owxphE7rlNL/v6tQXyxYz+3P5PPodLwEyNjxZKFMcYkqNOPbc9j\nY/rx6fqd3DtlEeVhFtKKFeuzMMaYBHZ5/1wKdxXz6MyV5LbJ5mcjjo9LHJYsjDEmwd15zrEU7S7m\nr++tJTcnmxtOPTrmMViyMMaYBCcijLvsBDbvLuaBV5bQuXVTzuvdKaYxWJ+FMcY0Ahnpafz5uoGc\n0KU1dz+/gEWFu2N6fksWxhjTSDRvksG/bhpM2+ZZ3DJxLgU7D8Ts3DbPwhhjGpk1W/cy6q8fk5Uh\nZKSl8eWeg/Uu7mjzLIwxJkn16NiSb516FNv2lrB5z0GUI2XNpy+oXlk3GixZGGNMI/Tygk3VtlWU\nNQ+CJQtjjGmENoWpnlvb9oayZGGMMY1QTeXLgyprbsnCGGMaoXuH9yI7M73StiCXurVJecYY0wjF\neqnbQJOFiIwA/g9IB/6pqo9Uef1oYALQAdgJ3KCqhd5rNwK/8Hb9tapOCjJWY4xpbEYOyI3ZGt2B\nNUOJSDrwBHAR0Ae4VkT6VNntMeBpVe0LjAPGe8e2BR4ETgGGAA+KSJugYjXGGFO7IPsshgBrVHWd\nqpYALwKXV9mnDzDbe/xuyOvDgVmqulNVdwGzgBEBxmqMMaYWQSaLXKAg5Hmhty3UQmCU9/gKoKWI\ntPN5rDHGmBgJMllImG1Va4vcA5wtIguAs4EioNTnsYjI7SKSLyL527Zta2i8xhhjahBksigEuoY8\nzwMqTTlU1U2qeqWqDgDu97bt8XOst++TqjpYVQd36NAh2vEbY4zxBFZIUEQygFXAebg7hrnAdaq6\nNGSf9sBOVS0XkYeBMlV9wOvgngcM9HadDwxS1Z21nG8b8EWEsNoD2+t7TUkgla8/la8dUvv67dpr\nd7SqRvxrO7Chs6paKiJ3AzNxQ2cnqOpSERkH5Kvqq8A5wHgRUWAOcJd37E4ReQiXYADG1ZYovGMi\nXqyI5PuprpisUvn6U/naIbWv3649Otce6DwLVZ0BzKiy7YGQx1OBqTUcOwE3B8MYY0ycWbkPY4wx\nEaVasngy3gHEWSpffypfO6T29du1R0HSrJRnjDEmOKl2Z2GMMaYeUiZZiMgIEVkpImtEZGy84wma\niEwQka0isiRkW1sRmSUiq73vSVlvS0S6isi7IrJcRJaKyP9425P++kWkqYh8JiILvWv/lbe9u4h8\n6l37v0UkK96xBkVE0kVkgYi87j1PpWvfICKLReRzEcn3tkXl9z4lkoXPoobJZiLV62mNBWarak9c\nTa5kTZqlwE9UtTdwKnCX9++dCtd/CBimqv2A/sAIETkV+C3wR+/adwG3xjHGoP0PsDzkeSpdO8C5\nqto/ZMhsVH7vUyJZ4K+oYVJR1Tm4su+hLgcqSr1PAkbGNKgYUdXNqjrfe7wX98GRSwpcvzr7vKeZ\n3pcCwzgyTD0prx1ARPKAS4B/es+FFLn2WkTl9z5VkoUVJnQ6qepmcB+oQMc4xxM4EekGDAA+JUWu\n32uG+RzYiqvYvBbYraql3i7J/Pv/OPBToNx73o7UuXZwfxi8LSLzROR2b1tUfu9TZaU8X4UJTXIR\nkRbAS8APVfUr90dm8lPVMqC/iOQALwO9w+0W26iCJyLfBLaq6jwROadic5hdk+7aQwxV1U0i0hGY\nJSIrovXGqXJn4aswYQrYIiKdAbzvW+McT2BEJBOXKJ5T1Wne5pS5fgBV3Q28h+u3yfHqtUHy/v4P\nBS4TkQ24puZhuDuNVLh2wBVn9b5vxf2hMIQo/d6nSrKYC/T0RkVkAdcAr8Y5pnh4FbjRe3wj8Eoc\nYwmM1079L2C5qv4h5KWkv34R6eDdUSAi2cD5uD6bd4HR3m5Jee2qep+q5qlqN9z/8XdU9XpS4NoB\nRKS5iLSseAxcCCwhSr/3KTMpT0Quxv2VUVHU8OE4hxQoEXkBV6ixPbAFt0ztdGAycBSwERgTqUBj\nYyQiZwD/BRZzpO3657h+i6S+fhHpi+vETMf9MThZVceJyDG4v7bbAgtw690fil+kwfKaoe5R1W+m\nyrV71/my9zQDeF5VH/YWlGvw733KJAtjjDH1lyrNUMYYYxrAkoUxxpiILFkYY4yJyJKFMcaYiCxZ\nGGOMiciShUk5ItIttBpvFN93nIicH2GfX4rIPbGKyZhoSZVyH8YELnR9+VgTkXSvzIcxgbA7C5PS\nROQYb+2Dk6tsP0dE3hORqSKyQkSe82aGIyKDROR9r1jbzJBSChNFZLT3+GLvuA9E5E8Vayt4+njv\nvU5EfhCyPUNEJonIIu+8zbz3Os+LcbG4dUqaeNs3iMgDIvIBMEZEfiAiy7zjXwzwx2ZSkCULk7JE\npBeuftTNqjo3zC4DgB/i1kA5Bhjq1Zz6f8BoVR0ETAAqVQMQkabA34GLVPUMoEOV9z0eGI6r2/Og\n954AvYAnVbUv8BVwp/deE4GrVfUkXGvA90Le66CqnqGqL+LWKRjgHX9HnX8gxtTCkoVJVR1wNXJu\nUNXPa9jnM1UtVNVy4HOgG+4D/URcRc/PgV/gitOFOh5Yp6rrvecvVHn9DVU9pKrbcUXdOnnbC1T1\nQ+/xs8AZ3vnWq+oqb/sk4KyQ9/p3yONFwHMicgNuAShjosb6LEyq2oNb42QosLSGfULrB5Xh/r8I\nsFRVT6vlvSPVQg/3vlC9dLb6eK/9IY8vwSWSy4D/FZETQtZxMKZB7M7CpKoS3Iph3xaR6+pw3Eqg\ng4icBq4UuoicUGWfFcAx3sJLAFf7fO+jKt4XuBb4wHuvbiLSw9v+LeD9qgeKSBrQVVXfxS3+kwO0\n8HleYyKyOwuTslR1v7dgziwR2a+qEUs3q2qJ14n9JxFpjfs/9DghdyeqWiwidwJvich24DOfIS0H\nbhSRvwOrgb+q6kERuRmY4q3JMBf4W5hj04FnvZgEt+b0bp/nNSYiqzprTABEpIWq7vNGUD0BrFbV\nP8Y7LmPqy5qhjAnGbV4H+FKgNW50lDGNlt1ZGGOMicjuLIwxxkRkycIYY0xEliyMMcZEZMnCGGNM\nRJYsjDHGRGTJwhhjTET/HxT/0MSnyHAQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d8cf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 50, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score = knn.score(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, 50, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 50, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=7 Test Acc: 0.958\n"
     ]
    }
   ],
   "source": [
    "# Note that k: 11 seems to be the best choice for this dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "print('k=7 Test Acc: %.3f' % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "X = cancer.drop(\"diagnosis\", axis=1)\n",
    "y = cancer[\"diagnosis\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Deep Learning Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='relu', input_dim=30))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 0s - loss: 0.5541 - acc: 0.7066\n",
      "Epoch 2/60\n",
      " - 0s - loss: 0.2582 - acc: 0.9343\n",
      "Epoch 3/60\n",
      " - 0s - loss: 0.1568 - acc: 0.9648\n",
      "Epoch 4/60\n",
      " - 0s - loss: 0.1131 - acc: 0.9789\n",
      "Epoch 5/60\n",
      " - 0s - loss: 0.0883 - acc: 0.9836\n",
      "Epoch 6/60\n",
      " - 0s - loss: 0.0751 - acc: 0.9859\n",
      "Epoch 7/60\n",
      " - 0s - loss: 0.0660 - acc: 0.9883\n",
      "Epoch 8/60\n",
      " - 0s - loss: 0.0594 - acc: 0.9883\n",
      "Epoch 9/60\n",
      " - 0s - loss: 0.0538 - acc: 0.9883\n",
      "Epoch 10/60\n",
      " - 0s - loss: 0.0496 - acc: 0.9930\n",
      "Epoch 11/60\n",
      " - 0s - loss: 0.0459 - acc: 0.9930\n",
      "Epoch 12/60\n",
      " - 0s - loss: 0.0424 - acc: 0.9930\n",
      "Epoch 13/60\n",
      " - 0s - loss: 0.0397 - acc: 0.9930\n",
      "Epoch 14/60\n",
      " - 0s - loss: 0.0367 - acc: 0.9930\n",
      "Epoch 15/60\n",
      " - 0s - loss: 0.0344 - acc: 0.9930\n",
      "Epoch 16/60\n",
      " - 0s - loss: 0.0304 - acc: 0.9930\n",
      "Epoch 17/60\n",
      " - 0s - loss: 0.0296 - acc: 0.9930\n",
      "Epoch 18/60\n",
      " - 0s - loss: 0.0269 - acc: 0.9930\n",
      "Epoch 19/60\n",
      " - 0s - loss: 0.0249 - acc: 0.9930\n",
      "Epoch 20/60\n",
      " - 0s - loss: 0.0233 - acc: 0.9953\n",
      "Epoch 21/60\n",
      " - 0s - loss: 0.0215 - acc: 0.9953\n",
      "Epoch 22/60\n",
      " - 0s - loss: 0.0199 - acc: 0.9953\n",
      "Epoch 23/60\n",
      " - 0s - loss: 0.0185 - acc: 0.9953\n",
      "Epoch 24/60\n",
      " - 0s - loss: 0.0173 - acc: 0.9953\n",
      "Epoch 25/60\n",
      " - 0s - loss: 0.0159 - acc: 0.9953\n",
      "Epoch 26/60\n",
      " - 0s - loss: 0.0145 - acc: 0.9953\n",
      "Epoch 27/60\n",
      " - 0s - loss: 0.0133 - acc: 0.9953\n",
      "Epoch 28/60\n",
      " - 0s - loss: 0.0123 - acc: 0.9953\n",
      "Epoch 29/60\n",
      " - 0s - loss: 0.0113 - acc: 0.9953\n",
      "Epoch 30/60\n",
      " - 0s - loss: 0.0108 - acc: 0.9953\n",
      "Epoch 31/60\n",
      " - 0s - loss: 0.0100 - acc: 0.9977\n",
      "Epoch 32/60\n",
      " - 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 33/60\n",
      " - 0s - loss: 0.0083 - acc: 0.9977\n",
      "Epoch 34/60\n",
      " - 0s - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 35/60\n",
      " - 0s - loss: 0.0073 - acc: 0.9977\n",
      "Epoch 36/60\n",
      " - 0s - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 37/60\n",
      " - 0s - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 38/60\n",
      " - 0s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 39/60\n",
      " - 0s - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 40/60\n",
      " - 0s - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 41/60\n",
      " - 0s - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 42/60\n",
      " - 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 43/60\n",
      " - 0s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 44/60\n",
      " - 0s - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 45/60\n",
      " - 0s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 46/60\n",
      " - 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 47/60\n",
      " - 0s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 48/60\n",
      " - 0s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 49/60\n",
      " - 0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 50/60\n",
      " - 0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 51/60\n",
      " - 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 52/60\n",
      " - 0s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 53/60\n",
      " - 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 54/60\n",
      " - 0s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 55/60\n",
      " - 0s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 56/60\n",
      " - 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 57/60\n",
      " - 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 58/60\n",
      " - 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 59/60\n",
      " - 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 60/60\n",
      " - 0s - loss: 0.0016 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114bf1fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.12330031737144748, Accuracy: 0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='sigmoid', input_dim=30))\n",
    "model.add(Dense(units=50, activation='sigmoid'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "426/426 [==============================] - 0s 768us/step - loss: 0.6366 - acc: 0.6338\n",
      "Epoch 2/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.5346 - acc: 0.6761\n",
      "Epoch 3/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.4346 - acc: 0.8826\n",
      "Epoch 4/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.3484 - acc: 0.9202\n",
      "Epoch 5/60\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.2771 - acc: 0.9272\n",
      "Epoch 6/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.2252 - acc: 0.9413\n",
      "Epoch 7/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.1861 - acc: 0.9460\n",
      "Epoch 8/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.1589 - acc: 0.9531\n",
      "Epoch 9/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.1383 - acc: 0.9601\n",
      "Epoch 10/60\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.1228 - acc: 0.9695\n",
      "Epoch 11/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.1086 - acc: 0.9812\n",
      "Epoch 12/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.1003 - acc: 0.9836\n",
      "Epoch 13/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0921 - acc: 0.9836\n",
      "Epoch 14/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0859 - acc: 0.9836\n",
      "Epoch 15/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0805 - acc: 0.9883\n",
      "Epoch 16/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0767 - acc: 0.9883\n",
      "Epoch 17/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0728 - acc: 0.9906\n",
      "Epoch 18/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0695 - acc: 0.9906\n",
      "Epoch 19/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0671 - acc: 0.9906\n",
      "Epoch 20/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0651 - acc: 0.9906\n",
      "Epoch 21/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0627 - acc: 0.9906\n",
      "Epoch 22/60\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0620 - acc: 0.9906\n",
      "Epoch 23/60\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0600 - acc: 0.9906\n",
      "Epoch 24/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0585 - acc: 0.9906\n",
      "Epoch 25/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0576 - acc: 0.9906\n",
      "Epoch 26/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0564 - acc: 0.9906\n",
      "Epoch 27/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0553 - acc: 0.9906\n",
      "Epoch 28/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0545 - acc: 0.9906\n",
      "Epoch 29/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0538 - acc: 0.9906\n",
      "Epoch 30/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0528 - acc: 0.9906\n",
      "Epoch 31/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0520 - acc: 0.9906\n",
      "Epoch 32/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0515 - acc: 0.9906\n",
      "Epoch 33/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0508 - acc: 0.9906\n",
      "Epoch 34/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0505 - acc: 0.9883\n",
      "Epoch 35/60\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0497 - acc: 0.9906\n",
      "Epoch 36/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0492 - acc: 0.9906\n",
      "Epoch 37/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0486 - acc: 0.9906\n",
      "Epoch 38/60\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0498 - acc: 0.9883\n",
      "Epoch 39/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0478 - acc: 0.9906\n",
      "Epoch 40/60\n",
      "426/426 [==============================] - 0s 58us/step - loss: 0.0473 - acc: 0.9883\n",
      "Epoch 41/60\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0468 - acc: 0.9906\n",
      "Epoch 42/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0463 - acc: 0.9906\n",
      "Epoch 43/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0460 - acc: 0.9906\n",
      "Epoch 44/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0459 - acc: 0.9906\n",
      "Epoch 45/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0454 - acc: 0.9906\n",
      "Epoch 46/60\n",
      "426/426 [==============================] - 0s 66us/step - loss: 0.0450 - acc: 0.9906\n",
      "Epoch 47/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0453 - acc: 0.9883\n",
      "Epoch 48/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0449 - acc: 0.9906\n",
      "Epoch 49/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0453 - acc: 0.9906\n",
      "Epoch 50/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0437 - acc: 0.9930\n",
      "Epoch 51/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0436 - acc: 0.9883\n",
      "Epoch 52/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0437 - acc: 0.9883\n",
      "Epoch 53/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0431 - acc: 0.9883\n",
      "Epoch 54/60\n",
      "426/426 [==============================] - 0s 96us/step - loss: 0.0431 - acc: 0.9883\n",
      "Epoch 55/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0425 - acc: 0.9883\n",
      "Epoch 56/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0437 - acc: 0.9883\n",
      "Epoch 57/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0421 - acc: 0.9883\n",
      "Epoch 58/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0419 - acc: 0.9883\n",
      "Epoch 59/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0417 - acc: 0.9930\n",
      "Epoch 60/60\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0409 - acc: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114c220f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.07555320741725968, Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}, Testing: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='linear', input_dim=30))\n",
    "model.add(Dense(units=50, activation='linear'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10e6349b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.18896039310931081, Accuracy: 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='tanh', input_dim=30))\n",
    "model.add(Dense(units=50, activation='tanh'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "426/426 [==============================] - 0s 919us/step - loss: 0.5362 - acc: 0.7207\n",
      "Epoch 2/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.1833 - acc: 0.9507\n",
      "Epoch 3/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.1163 - acc: 0.9718\n",
      "Epoch 4/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0927 - acc: 0.9789\n",
      "Epoch 5/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0785 - acc: 0.9836\n",
      "Epoch 6/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0713 - acc: 0.9836\n",
      "Epoch 7/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0653 - acc: 0.9859\n",
      "Epoch 8/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0616 - acc: 0.9859\n",
      "Epoch 9/60\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0579 - acc: 0.9883\n",
      "Epoch 10/60\n",
      "426/426 [==============================] - 0s 62us/step - loss: 0.0555 - acc: 0.9883\n",
      "Epoch 11/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.0290 - acc: 0.968 - 0s 72us/step - loss: 0.0541 - acc: 0.9883\n",
      "Epoch 12/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0506 - acc: 0.9906\n",
      "Epoch 13/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0498 - acc: 0.9906\n",
      "Epoch 14/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0485 - acc: 0.9906\n",
      "Epoch 15/60\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0470 - acc: 0.9906\n",
      "Epoch 16/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0461 - acc: 0.9906\n",
      "Epoch 17/60\n",
      "426/426 [==============================] - 0s 66us/step - loss: 0.0442 - acc: 0.9883\n",
      "Epoch 18/60\n",
      "426/426 [==============================] - 0s 60us/step - loss: 0.0445 - acc: 0.9906\n",
      "Epoch 19/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0419 - acc: 0.9906\n",
      "Epoch 20/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0412 - acc: 0.9930\n",
      "Epoch 21/60\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0401 - acc: 0.9930\n",
      "Epoch 22/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0387 - acc: 0.9906\n",
      "Epoch 23/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0369 - acc: 0.9906\n",
      "Epoch 24/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0373 - acc: 0.9930\n",
      "Epoch 25/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0363 - acc: 0.9930\n",
      "Epoch 26/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0360 - acc: 0.9930\n",
      "Epoch 27/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.0100 - acc: 1.000 - 0s 85us/step - loss: 0.0343 - acc: 0.9930\n",
      "Epoch 28/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0332 - acc: 0.9930\n",
      "Epoch 29/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0322 - acc: 0.9930\n",
      "Epoch 30/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0309 - acc: 0.9930\n",
      "Epoch 31/60\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0300 - acc: 0.9930\n",
      "Epoch 32/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0286 - acc: 0.9930\n",
      "Epoch 33/60\n",
      "426/426 [==============================] - 0s 113us/step - loss: 0.0281 - acc: 0.9930\n",
      "Epoch 34/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0275 - acc: 0.9930\n",
      "Epoch 35/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0263 - acc: 0.9930\n",
      "Epoch 36/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0252 - acc: 0.9930\n",
      "Epoch 37/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0239 - acc: 0.9930\n",
      "Epoch 38/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0237 - acc: 0.9930\n",
      "Epoch 39/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0217 - acc: 0.9930\n",
      "Epoch 40/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0211 - acc: 0.9953\n",
      "Epoch 41/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0200 - acc: 0.9953\n",
      "Epoch 42/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0194 - acc: 0.9953\n",
      "Epoch 43/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0182 - acc: 0.9953\n",
      "Epoch 44/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0176 - acc: 0.9953\n",
      "Epoch 45/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0167 - acc: 0.9953\n",
      "Epoch 46/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0157 - acc: 0.9953\n",
      "Epoch 47/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0151 - acc: 0.9953\n",
      "Epoch 48/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0147 - acc: 0.9953\n",
      "Epoch 49/60\n",
      "426/426 [==============================] - 0s 104us/step - loss: 0.0140 - acc: 0.9953\n",
      "Epoch 50/60\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0139 - acc: 0.9953\n",
      "Epoch 51/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0123 - acc: 0.9953\n",
      "Epoch 52/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0118 - acc: 0.9953\n",
      "Epoch 53/60\n",
      "426/426 [==============================] - 0s 101us/step - loss: 0.0112 - acc: 0.9953\n",
      "Epoch 54/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0108 - acc: 0.9953\n",
      "Epoch 55/60\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0100 - acc: 0.9953\n",
      "Epoch 56/60\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0088 - acc: 0.9977\n",
      "Epoch 58/60\n",
      "426/426 [==============================] - 0s 97us/step - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 59/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0083 - acc: 0.9977\n",
      "Epoch 60/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0083 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10e6343c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.09968050138452343, Accuracy: 0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (C4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90209790209790208"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ref: 21-2-4\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e21fb9ffcc05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tree.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path, f, prog)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m                 \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m             )\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, path, prog, format)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m                 \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[0;32m-> 1960\u001b[0;31m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprog\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "source": [
    "# WARNING! BOILERPLATE CODE HERE! \n",
    "# Use this to visualize the tree\n",
    "import graphviz \n",
    "target_names = [\"Benign\", \"Malignant\"]\n",
    "feature_names = data.columns\n",
    "dot_data = tree.export_graphviz(\n",
    "    clf, out_file=None, \n",
    "    feature_names=feature_names,  \n",
    "    class_names=target_names,  \n",
    "    filled=True, rounded=True,  \n",
    "    special_characters=True)  \n",
    "\n",
    "import pydotplus\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png('tree.png')\n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (NB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9389671361502347\n",
      "Testing Data Score: 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {model.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 22\n",
      "Best features : Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
      "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'radius_se', 'texture_se', 'area_se',\n",
      "       'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n",
      "       'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "clf_2 = RandomForestClassifier() \n",
    "rfecv = RFECV(estimator=clf_2, step=1, cv=5,scoring='accuracy') #5-fold cross-validation\n",
    "rfecv = rfecv.fit(X_train, y_train)\n",
    "\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', X_train.columns[rfecv.support_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEfCAYAAACwF+reAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xl4VOX1wPHvyQoh7IRFwg6ybyGi\nAgpaFdRaF5CKtnWte7VabdVabW3709ZqXapWq7iLC7igIojIIouy78gia1gT1gTIfn5/3BscQjJz\nk8xkZpLzeZ55MnPnzs25GZh37vu+57yiqhhjjDH+xIQ7AGOMMZHPGgtjjDEBWWNhjDEmIGssjDHG\nBGSNhTHGmICssTDGGBOQNRbGGGMCqlBjISKNRaRPqIIxxhgTmQI2FiIyQ0QaiEgTYBnwqog8GfrQ\njDHGRAovVxYNVfUQcBnwqqoOAM4JbVjGGGMiiZfGIk5EWgGjgc9CHI8xxpgI5KWxeASYAvygqgtE\npCOwPrRhGWOMiSRihQSNMcYE4mWA+2QRmSYiK93HfUTkwdCHZowxJlJ46Yb6H3A/UACgqsuBK0IZ\nlDHGmMjipbFIUtX5pbYVhiIYY4wxkclLY5ElIp0ABRCRUcDOkEZljDEmogQc4HZnP70EDAL2A5uA\nq1R1S+jDM8YYEwni/D0pIjFAuqqeIyL1gBhVza6e0IwxxkQKL1cWs1T1zGqKxxhjTATy0lj8CTgK\nvAccLtmuqvtCG5oxxphI4aWx2FTGZlXVjqEJyRhjTKSxDG5jjDEB+R3gBhCRX5W1XVXfCH44xhhj\nIlHAxgI4xed+HeAnwGLAGgtjjKklKtwNJSINgTdV9WehCckYY0ykqcwa3EeALsEOxBhjTOTyMmbx\nKW6pD5zGpQfwQSiDMsYYE1m8TJ0d6vOwENiiqhkhjcoYY0xE8dINdYGqznRvc1Q1Q0T+EfLIjDHG\nRAwvVxaLVTWt1LblqtonpJFVULNmzbR9+/bhDsMYY6LKokWLslQ1JdB+5Y5ZiMgtwK1ARxFZ7vNU\nfWBO1UMMrvbt27Nw4cJwh2GMMVFFRDxVEPc3wP0O8AXwKHCfz/ZsqwtljDG1S7mNhaoeBA4CYwBE\npDlOUl6yiCSr6tbqCdEYY0y4BRzgFpGLRGQ9zqJHM4HNOFccxhhjagkvs6H+BpwGrFPVDjjlPiJu\nzMIYY0zoeGksClR1LxAjIjGqOh3oF+K4jDHGRBAvhQQPiEgy8A3wtojswUnOM8YYU0t4ubK4GKce\n1G+BycAPwEWhDMoYY0xkCXhloaqHRaQd0EVVXxeRJCA29KEZY2qiFRkHOZRbwODOzcIdiqkAL7Oh\nfg2MB150N7UGPg5lUMaYmqmoWLl93GJueWsRuQVF4Q7HVICXbqjbgMHAIQBVXQ80D2VQxpia6as1\nu9my9wiHcgv5as3ucIdjKsBLY5GnqvklD0Qkjh9LlhtjjGevzN5E60Z1adWwDhMWWfHqaOKlsZgp\nIg8AdUXkXJy1LD4NbVjGmJpmRcZB5m/ax7WD23NJ/9bMWp/FnuzccIdlPPLSWNwHZAIrgJuAScCD\noQzKGFPzvDJ7I8mJcYw+pQ0j01IpKlY+WbIj3GEZj8ptLERkmnv3UVX9n6perqqj3PvWDWWM8Wzn\nwaN8tnwno9Pb0KBOPJ2bJ9O3TSMmLM7APk6ig78ri1buKnk/E5H+IpLme6uuAI0x0e/1uVsoVuXa\nwe2PbRuV1prvd2Wzaseh8AVmPPOXZ/EQThdUKvBkqecUODtUQRljao7DeYW8890WhvdsSZsmSce2\nX9T3JP762RomLM6gV+uGYYzQeFHulYWqjlfV84F/qupZpW7WUBhjPJmwOINDuYXccEaH47Y3Skrg\nnB7Nmbh0BwVFxWGKzngVcIBbVf9aHYEYY2qe4mJl7OxN9G3TiLS2jU94fmRaKnsP5zNjbWYYojMV\n4WU2lDHGVMq07/ewee8RbhjSARE54fkzT06hWXKC5VxEAWssjDEh88rsjbRuVJfze7Us8/n42Bgu\n7teaad/vZv/h/DL3MZHB39TZJv5u1RmkMSb6rNx+kG837uPqQe2Iiy3/e+nItFQKipRPl1vORSTz\nd2WxCFjo/swE1gHr3fuLQh+aMdFn1Y6DHM23AnnglPaolxDLz09p63e/Hic1oHurBtYVFeH8zYbq\noKodgSnARaraTFWbAj8FPqyuAI2JFjPW7uHCZ2Yz6LFpPDl1HXtz8sIdUtjsOpjLp8t2cHl6GxrW\njQ+4/8i01izLOMiGPdnVEJ2pDC9jFqeo6qSSB6r6BTA0dCEZE30Ki4r5v0lraNOkLgPaNeGZaesZ\n9NjXPPDRCjZm5oQ7vGr3xrzNFKly3eAOAfcFuLhfa2JjhPGLtoc2MFNpXhqLLBF5UETai0g7Efkj\nsDfUgRkTTcYvymDd7hweOL87L1+dzld3D+XS/q0ZvzCDnzw5kxvfWMiiLfvCHWa1OJJfyDvzt3Je\njxa0bZoU+AVASv1Ehp2cwkdLMigqtvIfkchLYzEGSAE+cm8p7jZjDE6G8hNT1zGgXWNGuLN+OjdP\n5rGRfZh931ncNqwz323ax8gX5nHZ83OYvHJXjf5AnLB4OweOFHDDGR0r9LqRA1LZfSiPORuyQhSZ\nqQovy6ruA+4UkWRVrX3X08YE8OKsjWRm5/HiLweckEvQvH4d7hnelVuGdeKDhdt4efYmbn5rER2a\n1eO+87sxvGfZU0qjVXGx8ursTfRJbUh6uxOT8Pz5SffmNKwbz4TFGZx5ckqIIjSV5WVZ1UEishpY\n7T7uKyLPhzwyY6LAroO5vDTrBy7s06rMDOUS9RLjuGZwB2bcM4z/XNmf2Bjhd+8vI6+wZs2cmr52\nDxuzDnN9OUl4/iTGxXJR31ZMWbWL7NyCEEVoKstLN9S/geG44xSqugw4M5RBGRMtnpy6luJiuG9E\nN0/7x8XG8NM+J/HHC7qTk1fI3A01a/jvldmbaNWwDhf0blWp149MSyW3oJhJK3YGOTJTVZ4yuFV1\nW6lNNevrkDGVsHrHIT5YlMHVg9odV03Vi0Gdm5KcGMfklbtCFF31W7XjIHN/2MvVg9oT7ycJz59+\nbRrRMaUeE2xWVMTx8o5uE5FBgIpIgojcA6wJcVzGRDRV5f8mraFh3XhuP6tLhV+fGBfL2d2aM3XN\nbgprSMXVV2ZvIikhljEBkvD8ERFGpqUyf/M+tuw9HMToTFV5aSxuBm4DWgMZQD/g1lAGZUykm7ku\nk9kbsrjj7C40TAqcdFaW83u1ZN/hfBZs3h+UmP7z9XpueH0h+YXV3/hk5eQ5SXgDUiv99yhxWVpr\nRJxZVYGoKjPW7uHqsfO5Y9wSimvwLLNw89JYdFXVq1S1hao2V9VfAN1DHZgxkaokAa9d0yR+cVq7\nSh9naNcUEuNimLKq6l1RR/OLeHHmRr5as5t/fbm2yserqOnf76GgSBl9SpsqH6tVw7oM7tSMDxdn\nlPvhn1tQxLj5Wznv37O45tUFLN12gInLdvDCzB+q/PtN2bw0Fs963GZMrfCBm4B334huJMRVvnBz\nUkIcQ09OYfLKXVX+Rvzl6l1k5xUyoF1jXpq1ka+/312l41XUrPVZpNRPpEerBkE53qgBqWTsP8r8\nzccnMmZm5/Hk1HUMeuxr7v9wBfGxMTw5ui8L/ngOF/ZpxZNT17Fwc+1Ifqxu/qrOni4ivwNSRORu\nn9ufgVgvBxeRESKyVkQ2iMh9ZTzfTkSmichyEZkhIqk+z7UVkS9FZI2IrBaR9hU+O2OC7HBeIU98\nuY50nwS8qhjRqyW7DuWyLONAlY4zflEGrRvV5a3rT6V7qwb87v1l7Dx4tMrxeVFUrHyzPpMzujSr\n8HTZ8gzv2ZLkxLhjxQXX7srm9+OXMfixr3lm2nrS2jZi3K9P4/M7hnBZWioJcTE8ellvWjeqyx3j\nlnDgiJU7DzZ/X4sSgGScxL36PrdDwKhABxaRWOA54HygBzBGRHqU2u1fwBuq2gd4BHjU57k3gMdV\ntTswENjj5YSMCaUXZ20kKyePP17YPSgfjD/p1oK4GGFyFbqidh48yuwNWYxMa03dhFj+c2V/8gqL\nuXPc0moZPF+x/SAHjhQwNIiJdHUTYrmgd0smrdjJL1/5juFPzWLish38/JQ2fP27obx89Smc3qnp\nce9BgzrxPDumP5k5efx+/HJUbfwimPxVnZ2pqn8BTlPVv/jcnlTV9R6OPRDYoKobVTUfeBe4uNQ+\nPYBp7v3pJc+7jUqcqk51Y8lR1SMVOzVjgqskAe+nfVrR308CXkU0TIrn9E5NmbJyV6U/3D5ash1V\np1wGQKeUZP5+aS/mb97HM9O8/FetmlnrMhGBM7oEN+t6dHobDucXsXZXNvcO78q8+37CXy/pRceU\n5HJf07dNI/4wohtfrt7N63M3BzWe2s5Lh+vLItKo5IGINBaRKR5e1xrwzc/IcLf5WgaMdO9fCtQX\nkabAycABEflQRJaIyOPulcpxRORGEVkoIgszM20NXxNaT3zpJOD9wWMCnlcjerVk894jrN1d8fLc\nqsqERRmc0r4x7ZrWO7b90v6pjBqQyrPTN4S81tLMdZn0ad2QJvUSgnrc9PZNmHrXmcz+w9ncdlZn\nGns8/vVDOnB2t+b836TvWbn9YFBjqs28NBbNVPVYh6qq7geae3hdWdfopb863QMMFZElOGXPtwOF\nOF1fZ7jPnwJ0BK454WCqL6lquqqmp6RYLRkTOqt3HGL84gyuGdy+wgl4gZzXoyUiVCpBb+m2A/yQ\neZiRaaknPPfIxT3p2Kwev31vKZnZoVlb4+CRApZs3R+yWk5dWtSv8CQCEeFfl/elSb0Ebn9nMTl5\nhSGJrbbx8i4Ui8ixLBsRaceJH/plyQB859GlAsetm6iqO1T1MlXtD/zR3XbQfe0StwurEPgYSPPw\nO40JOt8EvNuGdQ768VPqJ3JKuyaVaiwmLM6gTnwMF/Q5sbxGUkIcz12VxqGjBdz9/tKQ5CDM+SGL\nYiWo4xXB0KReAk9f0Y+t+47w4EcrbPwiCAJWncX5EJ8tIjPdx2cCN3p43QKgi4h0wLliuAK40ncH\nEWkG7FPVYuB+YKzPaxuLSIqqZgJn4yzxakxQHDxawLPT1pOdW0hcrBAfG0N8rBAXG+PcjxHi42KI\nixH2H8ln9oYsHvppjyonnJVneK+W/PWz1WzOOkz7ZvUCvwAn12Di0h0M79mSBnXKjqtbywY8fFFP\nHvhoBS/M/IHbzgpuYzdrXSb168TRr02jwDtXs1M7NuXOn5zMv79ax6DOzRidXvUckNrMS4nyySKS\nBpyG07V0l6oG7ARV1UIRuR1nWdZYYKyqrhKRR4CFqjoRGAY8KiIKzMLJFEdVi9yyItPEme6wCPhf\npc7QmFJ2H8rl6rHz2bAnh2bJiRQUFbs3pbDY+Vlap5R6VUrAC2R4zxb89bPVTFm1i5uGdvL0mmlr\n9nAot5BRA07sgvI1ZmAb5v6QxZNT13Fqhyakt28SjJBRVWauy2RI52bEVbIWVKjdfnZnvt24l4c/\nWUVa20Z0bl4/3CFFLQl0eeZ+WF8FdFTVR9wuqZaqOr86AvQqPT1dFy60iw/j38bMHH75ynwOHMnn\npV+lM7hzsxP2UVUKi/VYA1JQVEz9OnEkxnlKL6q0i56dTWyM8PFtgz3tf+2r81mzM5s5951NbIz/\nabzZuQX89NnZFBQW8/kdZ3geLPZn/e5szv33LB69rDdjBla+HlSo7T6UywVPf0Oz5EQ+uX0wdeJD\n+z5GGxFZpKrpgfbz8nXgeeB0flwdLxsnf8KYqLI84wCj/juP3IIi3r3x9DIbCnAGSONjY0hKiKNh\n3XiaJSeGvKEAZ1bU0m0HPCXT7cnOZdb6LC5Nax2woQCoXyee/4xJIzMnj3vHLwtKH/7Mdc4MxEhf\nqKhFgzo8Mbova3dn88hnq8MdTtTy0licqqq3AblwbDZUcOfIGRNis9dnMealb0lKiGX8LYPondow\n3CGdoGTVvC9XBS7V8cmSHRQVa5mzoMrTO7Uh95/fna/W7GHsnM2VDfOYmesy6dw8mdaN6lb5WKE2\nrGtzbjqzI+98t5XPl9taGZXhpbEocHMcFEBEUoCaUVO5lsstKOLeD5axvhLz+6PJp8t2cO1r82nT\nJIkJtwyig8cB5OrWuXkyXZonB5wVpaqMX5RBvzaN6Ny8/AS1slw7uD3ndG/BY1+sYc3OQ5WONbeg\niPmb9nFmkBPxQume4V3p16YR901Yzta9luNbUV4ai2eAj4DmIvJ3YDbwfyGNylSLCYsz+GBRBi/O\n2hjuUELmtTmbuOPdJfRv25j3bjqdFg3qhDskv0b0asl3m/ay73D5tY1W7TjE2t3ZAQe2yyIiPD6q\nD4lxsfy3ChVav924l7zCYoZ2jZ7GIj42hmfH9AeB28ctrnFL2oZawMZCVd8Gfo9Tt2kncImqfhDq\nwExoFRcrr8zeBMAXK3ZyJL9mJS6pKk98uZY/f7qac7u34I3rBtKwbmimvQbT8J4tKVb4anX5XVHj\nF2WQEBfDRX1OqtTvaFwvgStOacPny3ey40Dlig3OWpdFYlwMp3YIzsyq6tKmSRKPj+rL8oyDPDrp\n+3CHE1X8VZ1tUnLDKeI3DngH2O1uM1Fs5rpMNmYe5spT23I4v6hGLe9ZVKw88NFKnv16Az9Pb8Pz\nV6VFzQyYnic1ILVx3XILC+YXFvPJ0u2c26NFlXI+rhncnmLVStdPmrluD6d2bBo1f1dfI3q15NrB\n7Xlt7mYmr7TxC6/8XVkswkmEW+RzW+jz00Sxl2dvpGWDOjx8UQ/aNKnLhMUZ4Q4pKHILirj17UWM\nm7+V28/qzGMje0dsDkBZRIQRPVsye30W2bkFJzz/9fd72H+kgFEVGNguS2rjJM7v3Yp35m+tcDmM\njP1H+CHzMGd2KXs2WTS4//zu9E1tyL3jbfzCK39VZzuoakf3Z4dSjztWZ5AmuFbvOMScDXu5elB7\nEuNiuax/KnN/2FvpLolI8sCHK5iyajd/vqgH9wzvGrT1FarTiF4tyS8qZvraE4tjTlicQUr9RM4I\nwgf1DUM6kJ1byAcLtwXe2cesdU5O7rAoGq8oLSEuhv9c6VQQsvELbwJ+5RLHL0TkT+7jtiIyMPSh\nmVB5ZfYm6sbHcqWbSDUyLRVVp9R1NFu67QAfLtnOrcM6cc3gDuEOp9LS2jYmpX4iU0p1De7NyWP6\n93u4tH/roFwt9W/bmAHtGjN2ziaKKlA3ata6TE5qWIdOfkqFRwMbv6iYiiTlldR1sqS8KLbnUC4T\nl21ndHrqsT7vtk2TGNi+CRMWZ0RtwTVV5e+fr6ZZciK3Brn+UXWLiRGG92zB9LV7yC348RvvJ0t3\nUFjB3IpAbhjSgW37jjJ1tbcxq4KiYuZsyGJo15SovGor7fjxi5ozbhcKlpRXy7z57RYKi5VrS33z\nHjmgNRszD7N0W9WW9wyXKat2sWDzfu4+92SSE73Ux4xsI3q24kh+EbPW/dgVNWFxBr1bN6Rry+DV\nNzqvZ0vaNKnLy99s8rT/0m0HyM4rjKr8ikB+HL9YxrZ9Nn5RHkvKq0WO5hfx1rdbOLd7ixMqm17Q\nuxV14mOqfaA7GFcy+YXFPPbF93Rpnszo9OB96w6nUzs2oWHd+GOzotbsPMSqHYcYmVZ6/bCqiY0R\nrh3UgYVb9rNk6/6A+89cm0lsjDConFIp0ei48Yt3FpNfGF0fb9+sz6yWrHRLyqtFPlySwf4jBVw/\n5MT+/Pp14hnesyWfLttZbYN9j05awyXPzany4jRvfbuFzXuP8MCF3aNq5pM/8bExnNO9BV+t3k1B\nUTETFmUQHyv8rF9wGwuA0ae0oX6duGN5N/7MWp9J/zaNoiJnpSJKxi+WZRzk0S/WhDsczz5esp1r\nX13AS7N+qNC4U2VYUl4tUZKE17t1QwaWk0h1WVoqB48WMG3NnpDHs2bnIf73zUaWZRzk/g8rvzjN\nwSMFPPP1es7o0oxhEV7QrqJG9GrJodxCZq/P4uOl2zm7W/OgL10KkJwYx5UD2/LFyl1k7C+/G2Zv\nTh4rth+M+MKBlVUyfvHqnOgYv/jfrI389r2lpLdvzJs3nOqpoGRVeJkN1QnYpKrPASuBc33X5DbR\nYca6PWzMPMwNZ3Qod2BySOdmtGiQyIch7ooqWXmuQd14bh7aiU+X7eDNb7dU6lj/mb6eg0cLuP/8\n7jViwNXXGV2akZQQy18/W01WTn5QB7ZLu3pQewBe81NgcPaGLDQCV8ULpmgYvyguVv722Wr+PmkN\nF/ZuxevXDSx38atg8nLNPgEoEpHOwMtAB5xMbhNFXv5mEy0b1OGC3icuv1kiNka4pH9rZqzNJCsn\nNGs2A8xYl8k367O44+wu/H54V87qmsJfP1vNsgoOrm/de4TX527h8gGp9DipQYiiDZ868bGc1bU5\nG7MO07ReAmd1ax6y33VSo7pc2LsV7y7YVmYyIDhZ/42T4unVOvIq9gZLpI9f5BcWc9f7S3l59iau\nPr0dz4zpXy3l88HbsqrF7qp3lwFPq+qzIrIk1IGZ4Fm94xBzf9jLfed3Iz5An/6otFRenLmRT5bu\nKHNso6oKi4r5++dr6NDMWXkuJkZ4cnQ/fvrsbG59ezGf3zGERkneulr+Mfl7YmOE353XNehxRooR\nvVry+Yqd/KzfSQHfu6q64YwOTFy2g/cWbOOGM47Puy0uVmaty+KMLikh7+4It5Lxi5vfWsToF+fR\nvH5iwNckJ8bxu+FdQ1quPSevkJvfXMTsDVn8fkRXbhnaqVqvpr3OhhoD/Ar4zN1Ws0a3arhXZm8i\nKSGWMacEXs2sS4v69EltyIRFoemKGrdgGxv25HDf+d1IiHP++TWul8BzV6WxJzuX372/jGIPA3WL\ntuzj8xU7ufHMjhFfSbYqzunegqtObRuShru0PqmNGNi+Ca/O2Uxh0fHfqNfsOkRWTl6NHa8obUSv\nltx3fjdyC4rYuu9IwNuklTu57e3FFBSF5kokMzuPK16ax7yNe3l8VB9uHda52rtdvVxZXAvcDPxd\nVTeJSAfgrdCGZYKlJAnvqlPbeS48NzItlYcnrmLNzkN0bxW87p1DuQU85a4DfV6PFsc9169NI/54\nQXf+/OlqXpy1kVuGlb8Otaryt8/X0Lx+IjcNrdmVZ+omxPL3S3tX2++7/owO3PTmIqas2s2FfX7s\nsjy2Kl4U14OqqJuHduJmj+uhf7psB78Zt4Qnp67jDyO6BTWOzVmH+dXY+WRm5/Hyr9JD2h3pj5fZ\nUKtV9Q5VHec+3qSqj4U+NBMMb8wrScJr7/k1F/U9ifhYCfrVxfPTf2Dv4XwevLBHmd+Krh7Ungt7\nt+JfX67lu417yz3O5yt2smTrAe45rytJCdGfgBdJzunegnZNk3h59vFrnMxal0n3Vg1oXoOv4qri\nor4nccUpbXhhxg98s/7Eml6VtTzjACNfmEt2bgHv/PrUsDUU4K0bykSpo/lFvPWdk4TXrqn31eGa\n1EvgrK7N+XjpjhO6Iypr274jjJ2zicvSWpe7pKmI8NjI3rRtksRvxi0hM/vEQfa8wiL+Mfl7urWs\nz8hKLP5j/IuNEa4b3IElWw+waIuTpJeTV8jCzftr9CyoYHj4op50bp7MXe8tK/PfbkXNXJfJFS99\nS113KeD+bRsHIcrKs8aiBvtwSQYHjhScMFjpxcgBqWTl5PHN+qygxPLPKWuJEbh3uP/B6Pp14nn+\nqjQOHi3gzneXnJBo9MbcLWzbd5Q/Xti9xg+0hsuoAak0qBPHK+7Vxbwf9lJYrJx5cu3pgqqMugmx\n/OfK/mTnFnD3+0s9jb2V55Ol27n+tQW0a1qPD28ZFBFFG/0tfvSm+/PO6gvHBEtJEl6f1Iac0r7i\n30jO6tqcxknxjA9CzsXirfv5dNkObjyjI60aBp4t0r1VA/56SS/m/rCXp75ad2z7/sP5PPv1eoZ1\nTeGMGlSbKNLUS4zjylPbMXnlLrbtO8KsdZkkJcSS3s7WPAukW8sG/OmnPfhmfRb/+6ZyyxW/Pncz\nd767lAHtGvPeTadFTNefvyuLASLSDrhORBr7rpxnK+VFvpIkvOuHlJ+E509CXAwX92vN1NW7OXik\n7Hn3Xqg6CUQp9RO5yeNgIcDo9DZcPiCVZ7/ewIy1Tkb509PWk5NXyAMXdK90PMabawa1J0aEsXM2\nMXNdJoM6NT02e834d9WpbTm/V0sen7LWU72tEqrKv6eu4+GJqzi3R4tqS7bzyt+7/19gMtCN41fL\ns5XyosDL32yiVUP/SXiBjExLJb+wmM9W7Kj0MSat2MXirQe457yTqVfBarCPXNyLbi3rc9d7S5m7\nIYu3vt3Cz09py8ktgld11ZStZcM6XNT3JN7+bitb9x2x8YoKEBEeu6wPLRrU4Y53l3ConCRHX8XF\nysMTV/H0tPWMGpDKCxG4FLC/lfKeUdXuwNiyVsyrxhhrtD3ZuSyuwLcPL1btOMjcH/ZyzaD2VUrk\n6tW6ASe3SK70rKi8wiIem7yGbi3rM2pAmwq/vm5CLM9flUZBkfKLV74jMS6Gu87tUqlYTMVdP6TD\nsQzm2pJfESwNk+J5Zkw/dhzI5YEAtc/yC4v57XtLeWPeFm48syOPj+oTkQUxvUydvUVE+orI7e6t\nT3UEVlv86eOVXPb8XF6d4209gUD2H87ngY9WkpQQyxUDAyfh+SMijExLZfHWA2zMzKnw61+fu5lt\n+47y4IU9Kj0Y3TElmX+M7EOxwq1ndaZ5/cjov60NerVuyODOTencPLlCs+mMY0C7Jtx97sl8tnwn\n75ezdO2R/EJ+/cZCJi7bwX3nd+OBCyK3xpmXQoJ3AG8Dzd3b2yLym1AHVhscyS9kxtpM6teJ4y+f\nruapr9ZVaX2H7QeOMuq/c1mz8xD//nm/oJSRvqR/a2Kk4kuu7s3J49lpGzi7W3OGVDGR68I+rZj9\nh7O41U+ingmN568awDu/PjXcYUStm4d2YnDnpjw8cRXrd2cf99yBI/n84uXv+GZ9Jo9d1ttzAmC4\neLnWuQFntbyHVPUh4DTg16ENq3aYtS6TvMJiXrhqAKMGpPLUV+v5y6erKzXlbu2ubEY+P5c92Xm8\ned1AhvdsGZQYWzSow5AuKXxVDFDuAAAgAElEQVS4eHuF4np62nqOFBTxwAXByWZNbZwUsd+4arKG\ndePtaq4KYmOEf4/uR72EOH4zbsmxZXJ3Hcxl9IvzWLn9EM9flVblXoDq4KWxEMB3NZwid5uposkr\nd9E4KZ7TOjbhnyP7cP2QDrw2dzP3jF9WoWS4BZv3cfl/51Ksyvs3nc6pHZsGNc6Raa3ZfuAo324q\nP6va14Y9Obz93VauHNiWzs1tMNrUbs0b1OFfo/vy/a5s/vb5ajZlHWbUf+eyff9RXrv2FEb0qvwk\nlOrkZXrKq8B3IvKR+/gS4JXQhVQ75BcWM+37PZzfq+WxwawHL+xO46R4/vXlOg4dLeQ/V/YPOCNi\n6urd3P7OYlo3qsvr1w2kTZOkoMc6vGdL6ifG8dz0DWzKOkysCDEx4v6EGBFi3cciwlvfbiEpPpbf\nnmOD0caAk7d045kdeWmWU9E5PjaGcTeeRp/U6FkaKGBjoapPisgMYAjOFcW1quqpRLmIjACeBmKB\nl0vXlHLzOMYCKcA+4BeqmuHzfANgDfCRqt7u6YyixLyNe8nOLWRErx+7i0SE28/uQoO68Tz0ySqu\nfXUB/7s6neRyppy+t2Ar93+4gt6pjRh7dTpNkwOXUq6MOvGxjByQymtzNzNng7eriwcv7B6yeIyJ\nRvec15UFm/ex51Aeb1w/MCKysitCqjKg6vfAIrHAOuBcIANYAIxR1dU++3wAfKaqr4vI2TgN0S99\nnn8atyEJ1Fikp6frwoXRk/5x/4crmLh0O4v+dG6ZVw8fL9nO7z5YRs+TGvDatQOPW05TVXlu+gb+\n9eU6zjw5hReuSqtwDkNFqSpZOfkUq1KsSlGxUlwMRe7j4mKlyN2eGBdL5+bR9R/BmOqQX1iMotW2\nYJEXIrJIVdMD7RfKT5iBwAZV3egG9C5wMbDaZ58ewF3u/enAxyVPiMgAoAVOYmDAE4kmRcXK1NW7\nOatb83K7mS7p35rkxDhue2cxo1+cx5vXD6RVw7oUFSuPfLqK1+dt4dL+rfnnqD4hXxQHnKueFA+L\nwBhjyhfNWfChjLw14Du5OMPd5msZMNK9fylQX0SaikgM8ARwr79fICI3ishCEVmYmRm8ssChtnjr\nfrJy8gLOWDrHTfnfdTCXUS/MY+2ubO4Yt4TX3eSdJy7vWy0NhTHG+P2kEZFYEfmqkscua8ZU6T6v\ne4Ch7jKtQ4HtQCFwKzBJVcvOZCk5mOpLqpququkpKdGTYTp55S4SYmM81aY/rWNTxv36NI4WFDHi\n6Vl8vmInD1zgJO/EWNVVY0w18dsNpapFInJERBqq6sEKHjsD8K3xkAocV2RIVXcAlwGISDIwUlUP\nisjpwBkiciuQDCSISI6q3lfBGCKOqjJl1S6GdGlW7sB1ab1TG/L+TafzwIcrGHNqGy7tb+s4GGOq\nl5dPq1xghYhMBQ6XbFTVOwK8bgHQxV2GdTtwBXCl7w4i0gxn8LoYuB9nZhSqepXPPtcA6TWhoQBY\nteMQGfuPcsfZFZtW2rl5Mu/ffHqIojLGGP+8NBafu7cKUdVCEbkdmIIzdXasqq4SkUeAhao6ERgG\nPCoiCswCbqvo74k2X67aRYzAT7qHb3lEY4ypKE9TZ0WkLtBWVdeGPqTKiZaps+f9eyZN6iXw7o12\nlWCMCT+vU2e9FBK8CFiKM4UVEeknIhOrHmLtszEzh3W7cxgRpLpNxhhTXbzMu/wzTs7EAQBVXQp0\nCGFMNdaUVbsBOM8aC2NMlPHSWBSWMRMqNGnfNdzkVbvom9qQkxoFXofaGGMiiZfGYqWIXAnEikgX\nEXkWmBviuGqcnQePsmzbAbuqMMZEJS+NxW+AnkAeMA44BPw2lEHVRF+6XVC+hQONMSZaeKk6ewT4\no4j8w3mo2YFeY040ZdUuOjdPjrpKk8YYA95mQ50iIiuA5TjJecvcIn/Go32H8/lu0z6bBWWMiVpe\nkvJeAW5V1W8ARGQIzoJIfUIZWE3y1ZrdFBVr0JY6NcaY6uZlzCK7pKEAUNXZgHVFVcCXq3bRulFd\nerVuEO5QjDGmUsptLEQkTUTSgPki8qKIDBORoSLyPDCj2iKMIHM3ZPHYF9+TV1gUeGdXTl4hs9Zn\ncV7PFohYlVhjTHTy1w31RKnHD/vcr5V5FuMXZfDhku0s3bafF3+ZTsO68QFfM3NtJvmFxTZeYYyJ\nauU2Fqp6VnUGEg0yc/JonBTPoi37GfXCXF67biCtAyTYTV61i6b1Ekhv36SaojTGmODzMhuqkYjc\nISJPisgzJbfqCC7SZGbnkd6+Ca9f66xed+lzc1i1o/xlPvIKi5j+/R7O7dGCWFuoyBgTxbwMcE8C\n2gMrgEU+t1onKyePlPqJDOrcjPG3DCI2Rhj933nMXFf2kq5zN+wlJ6+Q4ZaIZ4yJcl4aizqqereq\nvqqqr5fcQh5ZhCksKmbv4XxSkhMB6NqyPh/dOpg2TZK47rUFvL/wxBVgJ6/cRXJiHIM6Na3ucI0x\nJqi8NBZvisivRaSViDQpuYU8sgiz73A+qtCsfuKxbS0b1uGDm09nUKem/H78cv49dR0l64MUFStT\n1+zm7G7NSYyLDVfYxhgTFF4ai3zgcWAeP3ZBRf4qQ0G2JzsP4NiVRYn6deIZe80pjBqQytPT1vP7\n8cspKCpmweZ97Ducb4l4xpgawUsG991AZ1XNCnUwkSwrx20s6iee8Fx8bAyPj+pD60Z1eXraenYd\nyqVVwzokxMUwrGtKdYdqjDFB56WxWAUcCXUgkS7TvbJoXkZjASAi3HXuybRuVJf7P1pBUbFyTvfm\n1Ev08ic2xpjI5uWTrAhYKiLTccqUA6Cqd4QsqgiU6V5ZNEsuu7EoMfqUNrRoWId7P1jGmIFtqyM0\nY4wJOS+NxcfurVbLzM6jfmIcdRMCD1YPPTmF+X88pxqiMsaY6uFlPYtaN022LJnZecfNhDLGmNok\nYGMhIpsooxaUqnYMSUQRKjM774SZUMYYU1t46YZK97lfB7gcqHV5Flk5eXRraSXGjTG1U8A8C1Xd\n63PbrqpPAWdXQ2wRJTM7r8xps8YYUxt46YZK83kYg3OlUT9kEUWg3IIiDuUWWmNhjKm1vHRD+a5r\nUQhsBkaHJJoIlXVs2mxCmCMxxpjw8DIbqtava1GSkGdXFsaY2spLN1QiMBKnTPmx/VX1kdCFFVmy\ncvIBSEmuE+ZIjDEmPLx0Q30CHMQpIJgXYN8aya4sjDG1nZfGIlVVR4Q8kghW0lg0tTELY0wt5aVE\n+VwR6V2Zg4vICBFZKyIbROS+Mp5vJyLTRGS5iMwQkVR3ez8RmSciq9znfl6Z3x8smTm5NE6KJz7W\ny5/LGGNqHi+ffkOARe6H/nIRWSEiywO9SERigeeA84EewBgR6VFqt38Bb6hqH+AR4FF3+xHgV6ra\nExgBPCUijbydUvBZjoUxprbz0g11fiWPPRDYoKobAUTkXeBiYLXPPj2Au9z703ELFqrqupIdVHWH\niOwBUoADlYylSrJy8q2xMMbUal4yuLeUdfNw7NaA78LUGe42X8twZloBXArUF5HjFqwWkYFAAvBD\n6V8gIjeKyEIRWZiZmekhpMqxulDGmNoulJ3wUsa20gUJ7wGGisgSYCiwHSfxzzmASCvgTeBaVS0+\n4WCqL6lquqqmp6SEZkU6VbVuKGNMrVduN5SIJKpqVabKZgBtfB6nAjt8d1DVHcBl7u9LBkaq6kH3\ncQPgc+BBVf22CnFUyeH8Io4WFAVc9MgYY2oyf1cW8wBE5M1KHnsB0EVEOohIAnAFMNF3BxFpJiIl\nMdwPjHW3JwAf4Qx+f1DJ3x8UlmNhjDH+B7gTRORqYJCIXFb6SVX90N+BVbVQRG4HpgCxwFhVXSUi\njwALVXUiMAx4VEQUmAXc5r58NHAm0FRErnG3XaOqS72fWnCU1IWyxsIYU5v5ayxuBq4CGgEXlXpO\nAb+NBYCqTgImldr2kM/98cD4Ml73FvBWoONXB7uyMMYYP42Fqs4GZovIQlV9pRpjiijHGgsbszDG\n1GJe8izeFJE7cLqFAGYC/1XVgtCFFTkys/OIjREaJ1mpD2NM7eWlsXgeiHd/AvwSeAG4IVRBRZLM\n7Dya1ksgJqasmcDGGFM7eGksTlHVvj6PvxaRZaEKKNJk5liOhTHGeEnKKxKRTiUPRKQjUBS6kCJL\nljUWxhjj6criXmC6iGzEycpuB1wb0qgiSGZ2Hl1b1Kolx40x5gRellWdJiJdgK44jcX3VczsjhrF\nxWpXFsYYg7crC9zGIWBZ8prm4NECCorUSn0YY2o9W83Hj0zL3jbGGMAaC7+yLHvbGGMAD42FOH4h\nIg+5j9u6a0zUeHZlYYwxDi9XFs8DpwNj3MfZOMul1nhWF8oYYxxeBrhPVdU0d4EiVHW/W0K8xsvM\nziMhLob6iZ7mARhjTI3l5cqiQERicVe5E5EU4IRV62qikuVURazUhzGmdvPSWDyDsxBRcxH5OzAb\n+L+QRhUhrNSHMcY4vCTlvS0ii4Cf4CTlXaKqa0IeWQTIzM6jTZOkcIdhjDFh57excJc8Xa6qvYDv\nqyekyJGVk0dau8bhDsMYY8LObzeUqhYDy0SkbTXFEzEKi4rZezjfsreNMQZvs6FaAatEZD5wuGSj\nqv4sZFFFgH2H81G1abPGGAPeGou/hDyKCHQsIc+uLIwxxtMA90wRaQGc4m6ar6p7QhtW+FlCnjHG\n/MhLuY/RwHzgcmA08J2IjAp1YOFW0lg0t8bCGGM8dUP9EWdp1T1wLCnvK2B8KAMLt5JuKBvgNsYY\nb0l5MaW6nfZ6fF1Uy8zOIzkxjroJseEOxRhjws7LlcVkEZkCjHMf/xz4InQhRYbMbMveNsaYEl4G\nuO8VkcuAITgZ3C+p6kchjyzMsnLybCaUMca4AjYWItIBmKSqH7qP64pIe1XdHOrgwikzO49uLRuE\nOwxjjIkIXsYePuD4KrNF7rYazbqhjDHmR14aizhVzS954N6v0etZ5BYUcSi3kGbJNfo0jTHGMy+N\nRaaIHCvtISIXA1mhCyn8smw5VWOMOY6X2VA3A2+LyH9wBri3Ab8KaVRhlpXjXEhZY2GMMY6AVxaq\n+oOqngb0AHqo6iBV3eDl4CIyQkTWisgGEbmvjOfbicg0EVkuIjNEJNXnuatFZL17u7oiJ1VVx0p9\nJNepzl9rjDERy0u5jztFpAFOxdl/i8hiETnPw+tigeeA83EamjEi0qPUbv8C3lDVPsAjwKPua5sA\nDwOnAgOBh0Wk2haWsLpQxhhzPC9jFtep6iHgPKA5cC3wmIfXDQQ2qOpGd1D8XeDiUvv0AKa596f7\nPD8cmKqq+1R1PzAVGOHhdwZFSWPR1Aa4jTEG8NZYiPvzAuBVVV3ms82f1jjjGyUy3G2+lgEj3fuX\nAvVFpKnH1yIiN4rIQhFZmJmZ6SEkbzJzcmmcFE98bI2vamKMMZ54+TRcJCJf4jQWU0SkPsfnXZSn\nrAZFSz2+BxgqIkuAocB2oNDja1HVl1Q1XVXTU1JSPITkTVZ2vnVBGWOMDy+zoa4H+gEbVfWI+83/\nWg+vywDa+DxOBXb47qCqO4DLAEQkGRipqgdFJAMYVuq1Mzz8zqDIzLGEPGOM8eVlNlSxqi5W1QPu\n472qutzDsRcAXUSkg4gkAFcAE313EJFmIlISw/3AWPf+FOA8EWnsDmyf526rFpnZVhfKGGN8haxT\nXlULgdtxPuTXAO+r6ioRecQnyW8YsFZE1gEtgL+7r90H/BWnwVkAPOJuCzlVJTM7z9axMMYYH166\noSpNVScBk0pte8jn/njKWURJVcfy45VGtTmcX8TRgiLrhjLGGB+eGgs3Z6KF7/6qujVUQYWT5VgY\nY8yJvJQo/w1OgtxufpwFpUCfEMYVNlYXyhhjTuTlyuJOoKuq7g11MJHAriyMMeZEXga4twEHQx1I\npChpLGyA2xhjfuTlymIjMENEPgfySjaq6pMhiyqMMrPziI0RGidZqQ9jjCnhpbHY6t4SqOGLHoHT\nWDStl0BsjJeKJsYYUzsEbCxU9S8AbpkPVdWckEcVRlmWvW2MMSfwUqK8l1u7aSWwSkQWiUjP0IcW\nHlbqwxhjTuRlgPsl4G5Vbaeq7YDfAf8LbVjhY6U+jDHmRF4ai3qqOr3kgarOAOqFLKIwKi5WsnLy\naGZXFsYYcxxPs6FE5E/Am+7jXwCbQhdS+Bw8WkBBkdqVhTHGlOJppTwgBfgQ+Mi976VEedSx7G1j\njCmbl9lQ+4E7qiGWsLPsbWOMKVu5jYWIPKWqvxWRTyl7lbqflfGyqJZpVxbGGFMmf1cWJWMU/6qO\nQCKBlfowxpiyldtYqOoi924/VX3a9zkRuROYGcrAwiEzO4+EuBga1AnpMh/GGBN1vAxwX13GtmuC\nHEdEyMxxcixErNSHMcb48jdmMQa4EuggIr5rZ9cHamS58sxsy942xpiy+OtvmQvsBJoBT/hszwaW\nhzKocMnMzqNNk6Rwh2GMMRHH35jFFmALcHr1hRNeWTl59G/bONxhGGNMxPFSSPA0EVkgIjkiki8i\nRSJyqDqCq06FRcXsPZxv3VDGGFMGLwPc/wHGAOuBusANwLOhDCoc9h3OR9VyLIwxpiye5oiq6gYR\niVXVIuBVEZkb4riq3bGEPMuxMMaYE3hpLI6ISAKwVET+iTPoXeOqzlqpD2OMKZ+XbqhfArHA7cBh\noA0wMpRBhcOxxsKuLIwx5gReCgluce8eBf4S2nDCp6Qbqln9Gr/MuDHGVJi/pLwVlFFAsISq9glJ\nRGGSmZ1HcmIcSQlW6sMYY0rz98n4U/fnbe7PksKCVwFHQhZRmGTl2LRZY4wpT6CkPERksKoO9nnq\nPhGZAzwS6uCqU2Z2ro1XGGNMOTytwS0iQ0oeiMggauhsKLuyMMaYsnnpoL8eGCsiDd3HB3CWWq1R\nMrPzGNLZBreNMaYsAa8sVHWRqvYF+gB9VbWfqi72cnARGSEia0Vkg4jcV8bzbUVkuogsEZHlInKB\nuz1eRF4XkRUiskZE7q/oiVVEbkERh3IL7crCGGPK4W821C9U9S0RubvUdgBU9Ul/BxaRWOA54Fwg\nA1ggIhNVdbXPbg8C76vqCyLSA5gEtAcuBxJVtbeIJAGrRWScqm6u6Al6sfdwPmAJecYYUx5/3VAl\n4xL1K3nsgcAGVd0IICLvAhcDvo2FAg3c+w2BHT7b64lIHE49qnwgZMULLXvbGGP88zcb6kX3Z2UT\n8VoD23weZwCnltrnz8CXIvIbnMbpHHf7eJyGZSeQBNylqvtK/wIRuRG4EaBt27aVDNM3e7tOpY9h\njDE1mb9uqGf8vVBV7whw7LLWJi2d5DcGeE1VnxCR04E3RaQXzlVJEXAS0Bj4RkS+KrlK8YnhJeAl\ngPT09HITCAMpaSwse9sYY8rmrxtqURWPnYFTR6pEKj92M5W4HhgBoKrzRKQOzsp8VwKTVbUA2OPm\ndaQDGwmBksaiaT3rhjLGmLL464Z6vYrHXgB0EZEOwHbgCpxGwNdW4CfAayLSHagDZLrbzxaRt3C6\noU4DnqpiPOXKysmjcVI8CXFe0k6MMab2CZhnISIpwB+AHjgf5gCo6tn+XqeqhSJyOzAFp2rtWFVd\nJSKPAAtVdSLwO+B/InIXThfVNaqqIvIc8CqwEqc761VVDdm635aQZ4wx/nlJynsbeA+4ELgZuBrn\n239AqjoJZzqs77aHfO6vBgaX8bocnOmz1SIzxxoLY4zxx0u/S1NVfQUoUNWZqnodTrdQjZGZnUcz\nqwtljDHl8nJlUeD+3CkiF+IMUqeGLqTqpapON5Q1FsYYUy4vjcXf3LpQvwOexUmiuyukUVWjw/lF\nHC0osm4oY4zxw0tj8Z2qHgQOAmeFOJ5qV1BYzEV9T6J7qwaBdzbGmFrKS2MxV0Q24Qxyf6iq+0Mc\nU7VqXC+BZ8f0D3cYxhgT0bxUne2CU/CvJ7BIRD4TkV+EPDJjjDERw1MWmqrOV9W7ccpw7AOqmrBn\njDEmigRsLESkgYhcLSJfAHNxivsNDHlkxhhjIoaXMYtlwMfAI6o6L8TxGGOMiUBeGouOqlrpiq7G\nGGOin5cBbmsojDGmlrMyq8YYYwKyxsIYY0xAEqiXSUT+CfwNOApMBvoCv1XVt0IfnncikglsKbW5\nGZAVhnBCqaadU007H6h551TTzgdq3jlV5XzaqWpKoJ28NBZLVbWfiFwKXIJTF2q6qvatZGDVRkQW\nqmp6uOMIppp2TjXtfKDmnVNNOx+oeedUHefjpRsq3v15ATBOVfeFMB5jjDERyMvU2U9F5Hucbqhb\n3ZXzckMbljHGmEjiZersfcDpQLqqFgCHgYtDHViQvBTuAEKgpp1TTTsfqHnnVNPOB2reOYX8fLyM\nWVwOTFbVbBF5EEgD/qaqi0MdnDHGmMjgZcziT25DMQQYjlNE8IXQhmWMMSaSeGksityfFwIvqOon\nQELoQjLGGBNpvDQW20XkRWA0MElEEj2+LqxEZISIrBWRDSJyX7jjqSoR2SwiK0RkqYgsDHc8lSEi\nY0Vkj4is9NnWRESmish692fjcMZYEeWcz59FZLv7Pi0VkQvCGWNFiUgbEZkuImtEZJWI3Oluj8r3\nyc/5RO37JCJ1RGS+iCxzz+kv7vYOIvKd+x69JyJB/VLvZcwiCRgBrFDV9SLSCuitql8GM5BgEpFY\nYB1wLpABLADGqOrqsAZWBSKyGWeSQdQmEonImUAO8Iaq9nK3/RPYp6qPuY16Y1X9Qzjj9Kqc8/kz\nkKOq/wpnbJXl/v9upaqLRaQ+sAgnv+oaovB98nM+o4nS90lEBKinqjkiEg/MBu4E7sZZzfRdEfkv\nsExVgzZk4GU21BHgB2C4iNwONI/khsI1ENigqhtVNR94l+iZwVVjqeosnMWzfF3Mj4tpvY7zHzkq\nlHM+UU1Vd5ZMXlHVbGAN0JoofZ/8nE/UUkeO+zDevSlwNjDe3R7098jL4kd3Am8Dzd3bWyLym2AG\nEQKtgW0+jzOI8n8gOP8YvhSRRSJyY7iDCaIWqroTnP/YOP/Got3tIrLc7aaKiu6asohIe6A/8B01\n4H0qdT4Qxe+TiMSKyFJgDzAV5wv9AVUtdHcJ+meel7GH64FTVfUhVX0IOA34dTCDCAEpY1u0l1of\nrKppwPnAbW4XiIk8LwCdgH44q0o+Ed5wKkdEkoEJOHXgDoU7nqoq43yi+n1S1SJV7Qek4vSkdC9r\nt2D+Ti+NhfDjjCjc+2V9GEeSDKCNz+NUYEeYYgkKVd3h/twDfETNWdp2t9uvXNK/vCfM8VSJqu52\n/yMXA/8jCt8ntx98AvC2qn7obo7a96ms86kJ7xOAqh4AZuB8iW8kIiVVOYL+meelsXgV+M6dPfBn\n4FvglWAGEQILgC7u7IAE4ApgYphjqjQRqecOziEi9YDzgJX+XxU1JgJXu/evBj4JYyxVVvKB6rqU\nKHuf3MHTV4A1qvqkz1NR+T6Vdz7R/D6JSIqINHLv1wXOwRmLmQ6McncL+nsUcDaUG1AaMATnimKW\nqi4JZhCh4E6FewqIBcaq6t/DHFKliUhHnKsJcOp5vRON5yMi44BhOOWUdwMP46zv/j7QFtgKXB4t\nxSrLOZ9hOF0bCmwGbirp648GbvLtN8AKoNjd/ABOP3/UvU9+zmcMUfo+iUgfnAHsWJwv/O+r6iPu\n58S7QBNgCfALVc0L2u/111iISAywvGRaoDHGmNrJbzeU25+3TETaVlM8xhhjIpCXEuWtgFUiMh+n\n4iwAqvqzkEVljDEmonhpLP4S8iiMMcZEtHIbCxHpjJOIM7PU9jOB7aEOzBhjTOTwN2bxFJBdxvYj\n7nPGGGNqCX+NRXtVXV56o6ouBNqHLCITtUREReQJn8f3uLk5wTj2ayIyKvCeVf49l7sVSqeX8dzj\nbpXPxytx3H6RXtlURHIC71Xm6y4RkR7V9ftMePhrLOr4ea5usAMxNUIecJmINAt3IL7cKsReXQ/c\nqqpnlfHcTUCaqt5biTD6ARVqLMQR8csB4BSsq3BjYaKLv3+IC0TkhBpQInI9TplfY0orxFkL+K7S\nT5S+Mij5Vikiw0Rkpoi8LyLrROQxEbnKrde/QkQ6+RzmHBH5xt3vp+7rY91v/AvconA3+Rx3uoi8\ng5OQVTqeMe7xV4rIP9xtD+Ekn/639NWDiEwE6uFUM/i5m0U7wf29C0RksLvfQBGZKyJL3J9d3SoC\njwA/F2fthJ+7FRHu8Tn+ShFp797WiMjzwGKgjYicJyLzRGSxiHzg1jnC/Vutds/7hFLbIjJUflyv\nYYlPFYB7ff5eZU5gKW8fEfmVu22ZiLwpIoOAnwGPu7+nk3ubLE7Ry29EpJv72g7ueSwQkb+W9XtN\nBFPVMm9AC2AuTt2RJ9zbTGAe0LK819mt9t5w1nZogJMR2xC4B/iz+9xrwCjffd2fw4ADOFO0E3Em\nT/zFfe5O4Cmf10/G+YLTBaf+Vx3gRuBBd59EYCHQwT3uYaBDGXGehJOFnIIzyeNr4BL3uRk464aU\neX4+998Bhrj32+KUk8A9/zj3/jnABPf+NcB/fF7/Z+Aen8crcbp32+NkGp/mbm8GzMJZvwDgD8BD\nOFm6a/kxsbZRGfF+ilOAEiDZPdfzcBp0cf+WnwFnlnpPytwH6On+zmbufk3KeW+nAV3c+6cCX7v3\nJwK/cu/f5vv3tFvk38qdDaWqu4FBInIWUJLB/bmqfl3ea4xR1UMi8gZwB3DU48sWqFtqQUR+AErW\nS1kB+HYHva9Oouh6EdkIdMP5YOvjc9XSEKcxyQfmq+qmMn7fKcAMVc10f+fbOB+GH3uMF5yGoIfI\nsZqaDdxv7g2B10WkC04pifgKHLPEFlX91r1/Gk4Xzxz3dyXgfGE7BOQCL4vI5zgf6KXNAZ50z+9D\nVc0QkfNw/mYlJXuScf5es3xeV94+fYHx6i7ApWWU+3CvegYBH/j8bRLdn4OBke79N4F/BPxLmIgR\nMM9CVafjFKgyxquncOTe02EAAAJMSURBVLpQXvXZVojb7SnOp4jvko++9WuKfR4Xc/y/0dK1aRTn\n2+9vVHWK7xMiMgyfJNJSglE1OQY4XVWPaxBF5FlguqpeKs76CTPKef2xv4fLd4zQN24BpqrqmNIH\nEJGBwE9wCmXejrP4zTHqrGr3Oc5Yybcico57vEdV9UU/51bmPiJyB4HLXsfgrKvQr5zno32pgFor\nGgbPTJRxv3G+jzNYXGIzMMC9fzGV+8Z9uYjEuOMYHXG6RKYAt4hThhoROVmcyrz+fAcMFZFm4gx+\nj8HpYq2IL3E+oHF/b8mHY0N+zEO6xmf/bKC+z+PNQJr72jScrrOyfAsMFifvCRFJcs8xGWioqpOA\n3+IMoB9HRDqp6gpV/QdO91w3nL/XdT7jHq1FpPRCRuXtMw0YLSJN3e1NSp+bOmtFbBKRy919RET6\nuvvNwWnYAK4q53xNhLLGwoTKEzj97SX+h/MBPR+nH7u8b/3+rMX5UP8CuFlVc4GXgdXAYhFZCbxI\ngCtmt8vrfpwr5mXAYlWtaDnnO4B0d7B3NXCzu/2fwKMiMgenKmiJ6TjdVktF5Oc46ys0EWe1s1tw\n1owvK9ZMnEZnnIgsx2k8uuF8OH/mbptJGZMKgN+6A+fLcLoEv1BnSeR3gHkisgJnGU7fRozy9lHV\nVcDfgZnuMUtKfr8L3OsOonfi/9u7QxuAYRiKgt61A2TQ4O4T4EqFPzTKHTZ/MrE7BM8389b/0nhU\nP+6a1VHlIFsnygG4m80CgEgsAIjEAoBILACIxAKASCwAiMQCgGgB6i0AYpgk0nMAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2638da20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 22 Features were selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_features= cancer.drop(cancer.columns[[9, 10, 13, 15, 16, 17, 25, 30 ]], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 22) (569,)\n"
     ]
    }
   ],
   "source": [
    "X = cancer_features.drop(\"diagnosis\", axis=1)\n",
    "y = cancer[\"diagnosis\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='relu', input_dim=22))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "426/426 [==============================] - 0s 1ms/step - loss: 0.4559 - acc: 0.8052\n",
      "Epoch 2/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.2253 - acc: 0.9554\n",
      "Epoch 3/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.1506 - acc: 0.9624\n",
      "Epoch 4/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.1140 - acc: 0.9765\n",
      "Epoch 5/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0933 - acc: 0.9812\n",
      "Epoch 6/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0790 - acc: 0.9859\n",
      "Epoch 7/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0694 - acc: 0.9883\n",
      "Epoch 8/60\n",
      "426/426 [==============================] - 0s 104us/step - loss: 0.0619 - acc: 0.9859\n",
      "Epoch 9/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0560 - acc: 0.9859\n",
      "Epoch 10/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.0448 - acc: 1.000 - 0s 78us/step - loss: 0.0506 - acc: 0.9906\n",
      "Epoch 11/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0464 - acc: 0.9906\n",
      "Epoch 12/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0429 - acc: 0.9906\n",
      "Epoch 13/60\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0394 - acc: 0.9906\n",
      "Epoch 14/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0373 - acc: 0.9906\n",
      "Epoch 15/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0342 - acc: 0.9906\n",
      "Epoch 16/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0319 - acc: 0.9906\n",
      "Epoch 17/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0298 - acc: 0.9906\n",
      "Epoch 18/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0276 - acc: 0.9930\n",
      "Epoch 19/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0260 - acc: 0.9930\n",
      "Epoch 20/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0244 - acc: 0.9930\n",
      "Epoch 21/60\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0228 - acc: 0.9930\n",
      "Epoch 22/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0209 - acc: 0.9930\n",
      "Epoch 23/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0197 - acc: 0.9930\n",
      "Epoch 24/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0184 - acc: 0.9930\n",
      "Epoch 25/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0170 - acc: 0.9930\n",
      "Epoch 26/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0155 - acc: 0.9953\n",
      "Epoch 27/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 28/60\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0135 - acc: 0.9977\n",
      "Epoch 29/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0126 - acc: 0.9977\n",
      "Epoch 30/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0120 - acc: 0.9977\n",
      "Epoch 31/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 32/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0106 - acc: 0.9977\n",
      "Epoch 33/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.0312 - acc: 0.968 - 0s 84us/step - loss: 0.0095 - acc: 0.9977\n",
      "Epoch 34/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0093 - acc: 0.9977\n",
      "Epoch 35/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 36/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0080 - acc: 0.9977\n",
      "Epoch 37/60\n",
      "426/426 [==============================] - 0s 100us/step - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 38/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 39/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 40/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 41/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 42/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 43/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 44/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 45/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 46/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 47/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 48/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 49/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "426/426 [==============================] - 0s 103us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 55/60\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 56/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 57/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.0050 - acc: 1.000 - 0s 72us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0023 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a25bc3f98>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.09953111227585688, Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning #Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='sigmoid', input_dim=22))\n",
    "model.add(Dense(units=50, activation='sigmoid'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "426/426 [==============================] - 0s 1ms/step - loss: 0.6100 - acc: 0.6268\n",
      "Epoch 2/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.4988 - acc: 0.7582\n",
      "Epoch 3/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.4004 - acc: 0.9178\n",
      "Epoch 4/60\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.3145 - acc: 0.9366\n",
      "Epoch 5/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.2461 - acc: 0.9484\n",
      "Epoch 6/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.1986 - acc: 0.9507\n",
      "Epoch 7/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.1675 - acc: 0.9624\n",
      "Epoch 8/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.1448 - acc: 0.9718\n",
      "Epoch 9/60\n",
      "426/426 [==============================] - 0s 98us/step - loss: 0.1298 - acc: 0.9789\n",
      "Epoch 10/60\n",
      "426/426 [==============================] - 0s 101us/step - loss: 0.1169 - acc: 0.9812\n",
      "Epoch 11/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.1078 - acc: 0.9765\n",
      "Epoch 12/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.1006 - acc: 0.9789\n",
      "Epoch 13/60\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0950 - acc: 0.9789\n",
      "Epoch 14/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0911 - acc: 0.9765\n",
      "Epoch 15/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0858 - acc: 0.9765\n",
      "Epoch 16/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0829 - acc: 0.9812\n",
      "Epoch 17/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0811 - acc: 0.9812\n",
      "Epoch 18/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0778 - acc: 0.9765\n",
      "Epoch 19/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0758 - acc: 0.9765\n",
      "Epoch 20/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0760 - acc: 0.9765\n",
      "Epoch 21/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0727 - acc: 0.9789\n",
      "Epoch 22/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0714 - acc: 0.9789\n",
      "Epoch 23/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0699 - acc: 0.9789\n",
      "Epoch 24/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0690 - acc: 0.9789\n",
      "Epoch 25/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0679 - acc: 0.9812\n",
      "Epoch 26/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0655 - acc: 0.9812\n",
      "Epoch 27/60\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0663 - acc: 0.9789\n",
      "Epoch 28/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.2271 - acc: 0.968 - 0s 90us/step - loss: 0.0640 - acc: 0.9812\n",
      "Epoch 29/60\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0638 - acc: 0.9812\n",
      "Epoch 30/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0629 - acc: 0.9812\n",
      "Epoch 31/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0631 - acc: 0.9836\n",
      "Epoch 32/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0620 - acc: 0.9859\n",
      "Epoch 33/60\n",
      "426/426 [==============================] - 0s 63us/step - loss: 0.0622 - acc: 0.9812\n",
      "Epoch 34/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0611 - acc: 0.9812\n",
      "Epoch 35/60\n",
      "426/426 [==============================] - 0s 66us/step - loss: 0.0602 - acc: 0.9836\n",
      "Epoch 36/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0590 - acc: 0.9859\n",
      "Epoch 37/60\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0586 - acc: 0.9859\n",
      "Epoch 38/60\n",
      "426/426 [==============================] - 0s 95us/step - loss: 0.0582 - acc: 0.9859\n",
      "Epoch 39/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0577 - acc: 0.9859\n",
      "Epoch 40/60\n",
      "426/426 [==============================] - 0s 104us/step - loss: 0.0571 - acc: 0.9859\n",
      "Epoch 41/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0570 - acc: 0.9859\n",
      "Epoch 42/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0566 - acc: 0.9859\n",
      "Epoch 43/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0563 - acc: 0.9859\n",
      "Epoch 44/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0556 - acc: 0.9859\n",
      "Epoch 45/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0556 - acc: 0.9859\n",
      "Epoch 46/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0550 - acc: 0.9859\n",
      "Epoch 47/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0546 - acc: 0.9859\n",
      "Epoch 48/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0544 - acc: 0.9859\n",
      "Epoch 49/60\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0547 - acc: 0.9859\n",
      "Epoch 50/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0539 - acc: 0.9859\n",
      "Epoch 51/60\n",
      "426/426 [==============================] - 0s 95us/step - loss: 0.0539 - acc: 0.9859\n",
      "Epoch 52/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0533 - acc: 0.9859\n",
      "Epoch 53/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0531 - acc: 0.9859\n",
      "Epoch 54/60\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0533 - acc: 0.9859\n",
      "Epoch 55/60\n",
      "426/426 [==============================] - 0s 101us/step - loss: 0.0522 - acc: 0.9859\n",
      "Epoch 56/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0521 - acc: 0.9859\n",
      "Epoch 57/60\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0522 - acc: 0.9859\n",
      "Epoch 58/60\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0516 - acc: 0.9859\n",
      "Epoch 59/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0520 - acc: 0.9883\n",
      "Epoch 60/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0512 - acc: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a25f32e10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.06917847549848884, Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
