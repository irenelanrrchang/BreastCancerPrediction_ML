{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and check the data\n",
    "cancer = pd.read_csv('../Resources/data.csv')\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data\n",
    "cancer = cancer.iloc[:,:-1]\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302        17.99         10.38          122.80     1001.0   \n",
       "1    842517        20.57         17.77          132.90     1326.0   \n",
       "2  84300903        19.69         21.25          130.00     1203.0   \n",
       "3  84348301        11.42         20.38           77.58      386.1   \n",
       "4  84358402        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean           ...             radius_worst  texture_worst  \\\n",
       "0         0.2419           ...                    25.38          17.33   \n",
       "1         0.1812           ...                    24.99          23.41   \n",
       "2         0.2069           ...                    23.57          25.53   \n",
       "3         0.2597           ...                    14.91          26.50   \n",
       "4         0.1809           ...                    22.54          16.67   \n",
       "\n",
       "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0           184.60      2019.0            0.1622             0.6656   \n",
       "1           158.80      1956.0            0.1238             0.1866   \n",
       "2           152.50      1709.0            0.1444             0.4245   \n",
       "3            98.87       567.7            0.2098             0.8663   \n",
       "4           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0           0.7119                0.2654          0.4601   \n",
       "1           0.2416                0.1860          0.2750   \n",
       "2           0.4504                0.2430          0.3613   \n",
       "3           0.6869                0.2575          0.6638   \n",
       "4           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ref: 21-2-6\n",
    "# Data Pre Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "target = cancer[\"diagnosis\"]\n",
    "data = cancer.drop(\"diagnosis\", axis=1)\n",
    "feature_names = data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1, Train/Test Score: 1.000/0.769\n",
      "k: 3, Train/Test Score: 0.911/0.755\n",
      "k: 5, Train/Test Score: 0.845/0.734\n",
      "k: 7, Train/Test Score: 0.793/0.748\n",
      "k: 9, Train/Test Score: 0.765/0.727\n",
      "k: 11, Train/Test Score: 0.735/0.692\n",
      "k: 13, Train/Test Score: 0.737/0.692\n",
      "k: 15, Train/Test Score: 0.730/0.678\n",
      "k: 17, Train/Test Score: 0.725/0.678\n",
      "k: 19, Train/Test Score: 0.718/0.685\n",
      "k: 21, Train/Test Score: 0.709/0.706\n",
      "k: 23, Train/Test Score: 0.702/0.713\n",
      "k: 25, Train/Test Score: 0.714/0.713\n",
      "k: 27, Train/Test Score: 0.700/0.699\n",
      "k: 29, Train/Test Score: 0.695/0.699\n",
      "k: 31, Train/Test Score: 0.697/0.699\n",
      "k: 33, Train/Test Score: 0.692/0.692\n",
      "k: 35, Train/Test Score: 0.688/0.699\n",
      "k: 37, Train/Test Score: 0.690/0.699\n",
      "k: 39, Train/Test Score: 0.688/0.699\n",
      "k: 41, Train/Test Score: 0.678/0.671\n",
      "k: 43, Train/Test Score: 0.678/0.671\n",
      "k: 45, Train/Test Score: 0.671/0.671\n",
      "k: 47, Train/Test Score: 0.650/0.664\n",
      "k: 49, Train/Test Score: 0.650/0.664\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noCxZGGOMCer/AdDVzikipeJ0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a23519e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 50, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score = knn.score(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, 50, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 50, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=7 Test Acc: 0.748\n"
     ]
    }
   ],
   "source": [
    "# Note that k: 11 seems to be the best choice for this dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "print('k=7 Test Acc: %.3f' % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31) (569,)\n"
     ]
    }
   ],
   "source": [
    "X = cancer.drop(\"diagnosis\", axis=1)\n",
    "y = cancer[\"diagnosis\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Deep Learning Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='relu', input_dim=31))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      " - 0s - loss: 0.4951 - acc: 0.7582\n",
      "Epoch 2/60\n",
      " - 0s - loss: 0.2494 - acc: 0.9343\n",
      "Epoch 3/60\n",
      " - 0s - loss: 0.1633 - acc: 0.9460\n",
      "Epoch 4/60\n",
      " - 0s - loss: 0.1203 - acc: 0.9577\n",
      "Epoch 5/60\n",
      " - 0s - loss: 0.0960 - acc: 0.9742\n",
      "Epoch 6/60\n",
      " - 0s - loss: 0.0823 - acc: 0.9718\n",
      "Epoch 7/60\n",
      " - 0s - loss: 0.0731 - acc: 0.9765\n",
      "Epoch 8/60\n",
      " - 0s - loss: 0.0651 - acc: 0.9765\n",
      "Epoch 9/60\n",
      " - 0s - loss: 0.0611 - acc: 0.9812\n",
      "Epoch 10/60\n",
      " - 0s - loss: 0.0543 - acc: 0.9859\n",
      "Epoch 11/60\n",
      " - 0s - loss: 0.0501 - acc: 0.9859\n",
      "Epoch 12/60\n",
      " - 0s - loss: 0.0459 - acc: 0.9883\n",
      "Epoch 13/60\n",
      " - 0s - loss: 0.0430 - acc: 0.9883\n",
      "Epoch 14/60\n",
      " - 0s - loss: 0.0386 - acc: 0.9906\n",
      "Epoch 15/60\n",
      " - 0s - loss: 0.0358 - acc: 0.9906\n",
      "Epoch 16/60\n",
      " - 0s - loss: 0.0328 - acc: 0.9930\n",
      "Epoch 17/60\n",
      " - 0s - loss: 0.0309 - acc: 0.9930\n",
      "Epoch 18/60\n",
      " - 0s - loss: 0.0281 - acc: 0.9930\n",
      "Epoch 19/60\n",
      " - 0s - loss: 0.0260 - acc: 0.9930\n",
      "Epoch 20/60\n",
      " - 0s - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 21/60\n",
      " - 0s - loss: 0.0229 - acc: 0.9930\n",
      "Epoch 22/60\n",
      " - 0s - loss: 0.0231 - acc: 0.9953\n",
      "Epoch 23/60\n",
      " - 0s - loss: 0.0212 - acc: 0.9930\n",
      "Epoch 24/60\n",
      " - 0s - loss: 0.0177 - acc: 0.9953\n",
      "Epoch 25/60\n",
      " - 0s - loss: 0.0163 - acc: 0.9953\n",
      "Epoch 26/60\n",
      " - 0s - loss: 0.0152 - acc: 0.9977\n",
      "Epoch 27/60\n",
      " - 0s - loss: 0.0137 - acc: 0.9977\n",
      "Epoch 28/60\n",
      " - 0s - loss: 0.0135 - acc: 0.9977\n",
      "Epoch 29/60\n",
      " - 0s - loss: 0.0127 - acc: 0.9977\n",
      "Epoch 30/60\n",
      " - 0s - loss: 0.0114 - acc: 0.9977\n",
      "Epoch 31/60\n",
      " - 0s - loss: 0.0107 - acc: 0.9977\n",
      "Epoch 32/60\n",
      " - 0s - loss: 0.0100 - acc: 0.9977\n",
      "Epoch 33/60\n",
      " - 0s - loss: 0.0092 - acc: 1.0000\n",
      "Epoch 34/60\n",
      " - 0s - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 35/60\n",
      " - 0s - loss: 0.0075 - acc: 0.9977\n",
      "Epoch 36/60\n",
      " - 0s - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 37/60\n",
      " - 0s - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 38/60\n",
      " - 0s - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 39/60\n",
      " - 0s - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 40/60\n",
      " - 0s - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 41/60\n",
      " - 0s - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 42/60\n",
      " - 0s - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 43/60\n",
      " - 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 44/60\n",
      " - 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 45/60\n",
      " - 0s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 46/60\n",
      " - 0s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 47/60\n",
      " - 0s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 48/60\n",
      " - 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 49/60\n",
      " - 0s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 50/60\n",
      " - 0s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 51/60\n",
      " - 0s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 52/60\n",
      " - 0s - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 53/60\n",
      " - 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 54/60\n",
      " - 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 55/60\n",
      " - 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 56/60\n",
      " - 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 57/60\n",
      " - 0s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 58/60\n",
      " - 0s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 59/60\n",
      " - 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 60/60\n",
      " - 0s - loss: 0.0019 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a23519630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.12388913922788904, Accuracy: 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='sigmoid', input_dim=31))\n",
    "model.add(Dense(units=50, activation='sigmoid'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "426/426 [==============================] - 0s 746us/step - loss: 0.6297 - acc: 0.6268\n",
      "Epoch 2/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.5400 - acc: 0.7136\n",
      "Epoch 3/60\n",
      "426/426 [==============================] - 0s 62us/step - loss: 0.4444 - acc: 0.8568\n",
      "Epoch 4/60\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.3539 - acc: 0.9014\n",
      "Epoch 5/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.2774 - acc: 0.9249\n",
      "Epoch 6/60\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.2224 - acc: 0.9343\n",
      "Epoch 7/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.1827 - acc: 0.9413\n",
      "Epoch 8/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.1530 - acc: 0.9460\n",
      "Epoch 9/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.1320 - acc: 0.9671\n",
      "Epoch 10/60\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.1169 - acc: 0.9742\n",
      "Epoch 11/60\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.1054 - acc: 0.9812\n",
      "Epoch 12/60\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0961 - acc: 0.9812\n",
      "Epoch 13/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0897 - acc: 0.9836\n",
      "Epoch 14/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0842 - acc: 0.9859\n",
      "Epoch 15/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0792 - acc: 0.9859\n",
      "Epoch 16/60\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0760 - acc: 0.9883\n",
      "Epoch 17/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0726 - acc: 0.9859\n",
      "Epoch 18/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0702 - acc: 0.9906\n",
      "Epoch 19/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0681 - acc: 0.9906\n",
      "Epoch 20/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0660 - acc: 0.9906\n",
      "Epoch 21/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0638 - acc: 0.9906\n",
      "Epoch 22/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0620 - acc: 0.9906\n",
      "Epoch 23/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0608 - acc: 0.9906\n",
      "Epoch 24/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0596 - acc: 0.9906\n",
      "Epoch 25/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0586 - acc: 0.9906\n",
      "Epoch 26/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0574 - acc: 0.9906\n",
      "Epoch 27/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0564 - acc: 0.9906\n",
      "Epoch 28/60\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0560 - acc: 0.9906\n",
      "Epoch 29/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0549 - acc: 0.9906\n",
      "Epoch 30/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0541 - acc: 0.9906\n",
      "Epoch 31/60\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0536 - acc: 0.9906\n",
      "Epoch 32/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0527 - acc: 0.9906\n",
      "Epoch 33/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0521 - acc: 0.9906\n",
      "Epoch 34/60\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0514 - acc: 0.9906\n",
      "Epoch 35/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0512 - acc: 0.9906\n",
      "Epoch 36/60\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0510 - acc: 0.9906\n",
      "Epoch 37/60\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0500 - acc: 0.9906\n",
      "Epoch 38/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0494 - acc: 0.9906\n",
      "Epoch 39/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0492 - acc: 0.9906\n",
      "Epoch 40/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0488 - acc: 0.9906\n",
      "Epoch 41/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0482 - acc: 0.9906\n",
      "Epoch 42/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0478 - acc: 0.9906\n",
      "Epoch 43/60\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0483 - acc: 0.9906\n",
      "Epoch 44/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0472 - acc: 0.9883\n",
      "Epoch 45/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0467 - acc: 0.9906\n",
      "Epoch 46/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0476 - acc: 0.9906\n",
      "Epoch 47/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0463 - acc: 0.9930\n",
      "Epoch 48/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0458 - acc: 0.9906\n",
      "Epoch 49/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0457 - acc: 0.9906\n",
      "Epoch 50/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0449 - acc: 0.9906\n",
      "Epoch 51/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0447 - acc: 0.9906\n",
      "Epoch 52/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0442 - acc: 0.9906\n",
      "Epoch 53/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0443 - acc: 0.9906\n",
      "Epoch 54/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0441 - acc: 0.9906\n",
      "Epoch 55/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0438 - acc: 0.9930\n",
      "Epoch 56/60\n",
      "426/426 [==============================] - 0s 108us/step - loss: 0.0435 - acc: 0.9930\n",
      "Epoch 57/60\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0431 - acc: 0.9930\n",
      "Epoch 58/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0430 - acc: 0.9930\n",
      "Epoch 59/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0422 - acc: 0.9906\n",
      "Epoch 60/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0433 - acc: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a23519320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.07438876152435851, Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='linear', input_dim=31))\n",
    "model.add(Dense(units=50, activation='linear'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c213898>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.19675358349757682, Accuracy: 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Methodology_#Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, activation='tanh', input_dim=31))\n",
    "model.add(Dense(units=50, activation='tanh'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "426/426 [==============================] - 1s 1ms/step - loss: 0.5662 - acc: 0.6972\n",
      "Epoch 2/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.1719 - acc: 0.9554\n",
      "Epoch 3/60\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.1137 - acc: 0.9718\n",
      "Epoch 4/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0912 - acc: 0.9812\n",
      "Epoch 5/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0798 - acc: 0.9859\n",
      "Epoch 6/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0724 - acc: 0.9836\n",
      "Epoch 7/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0674 - acc: 0.9812\n",
      "Epoch 8/60\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0633 - acc: 0.9836\n",
      "Epoch 9/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0591 - acc: 0.9836\n",
      "Epoch 10/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0569 - acc: 0.9836\n",
      "Epoch 11/60\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0545 - acc: 0.9859\n",
      "Epoch 12/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0515 - acc: 0.9859\n",
      "Epoch 13/60\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0502 - acc: 0.9859\n",
      "Epoch 14/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0489 - acc: 0.9859\n",
      "Epoch 15/60\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.1213 - acc: 0.968 - 0s 81us/step - loss: 0.0463 - acc: 0.9930\n",
      "Epoch 16/60\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0453 - acc: 0.9859\n",
      "Epoch 17/60\n",
      "426/426 [==============================] - 0s 99us/step - loss: 0.0451 - acc: 0.9859\n",
      "Epoch 18/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0423 - acc: 0.9906\n",
      "Epoch 19/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0419 - acc: 0.9906\n",
      "Epoch 20/60\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0407 - acc: 0.9906\n",
      "Epoch 21/60\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0390 - acc: 0.9906\n",
      "Epoch 22/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0377 - acc: 0.9906\n",
      "Epoch 23/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0373 - acc: 0.9930\n",
      "Epoch 24/60\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0360 - acc: 0.9930\n",
      "Epoch 25/60\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0349 - acc: 0.9930\n",
      "Epoch 26/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0343 - acc: 0.9906\n",
      "Epoch 27/60\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0334 - acc: 0.9906\n",
      "Epoch 28/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0321 - acc: 0.9930\n",
      "Epoch 29/60\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0306 - acc: 0.9906\n",
      "Epoch 30/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0305 - acc: 0.9906\n",
      "Epoch 31/60\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0277 - acc: 0.9953\n",
      "Epoch 32/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0277 - acc: 0.9930\n",
      "Epoch 33/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0266 - acc: 0.9930\n",
      "Epoch 34/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0256 - acc: 0.9953\n",
      "Epoch 35/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0242 - acc: 0.9930\n",
      "Epoch 36/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0238 - acc: 0.9930\n",
      "Epoch 37/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0226 - acc: 0.9953\n",
      "Epoch 38/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0214 - acc: 0.9953\n",
      "Epoch 39/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0211 - acc: 0.9953\n",
      "Epoch 40/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0198 - acc: 0.9953\n",
      "Epoch 41/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0208 - acc: 0.9930\n",
      "Epoch 42/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0189 - acc: 0.9953\n",
      "Epoch 43/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0208 - acc: 0.9953\n",
      "Epoch 44/60\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 45/60\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0155 - acc: 0.9953\n",
      "Epoch 46/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0155 - acc: 0.9953\n",
      "Epoch 47/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0141 - acc: 0.9953\n",
      "Epoch 48/60\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0134 - acc: 0.9953\n",
      "Epoch 49/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0128 - acc: 0.9953\n",
      "Epoch 50/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0124 - acc: 0.9977\n",
      "Epoch 51/60\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0123 - acc: 0.9977\n",
      "Epoch 52/60\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 53/60\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0106 - acc: 0.9953\n",
      "Epoch 54/60\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0102 - acc: 0.9977\n",
      "Epoch 55/60\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 56/60\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0091 - acc: 0.9977\n",
      "Epoch 57/60\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0089 - acc: 0.9977\n",
      "Epoch 58/60\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 59/60\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0081 - acc: 0.9977\n",
      "Epoch 60/60\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0077 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10ff74f98>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.11834168518096423, Accuracy: 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# Quantify our Trained Model\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (C4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92307692307692313"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ref: 21-2-4\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e21fb9ffcc05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tree.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path, f, prog)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m                 \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m             )\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, path, prog, format)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m                 \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[0;32m-> 1960\u001b[0;31m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprog\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "source": [
    "# WARNING! BOILERPLATE CODE HERE! \n",
    "# Use this to visualize the tree\n",
    "import graphviz \n",
    "target_names = [\"Benign\", \"Malignant\"]\n",
    "feature_names = data.columns\n",
    "dot_data = tree.export_graphviz(\n",
    "    clf, out_file=None, \n",
    "    feature_names=feature_names,  \n",
    "    class_names=target_names,  \n",
    "    filled=True, rounded=True,  \n",
    "    special_characters=True)  \n",
    "\n",
    "import pydotplus\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png('tree.png')\n",
    "\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (NB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('cancer').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+------------+--------------+---------+---------------+----------------+--------------+-------------------+-------------+----------------------+---------+----------+------------+-------+-------------+--------------+------------+-----------------+-----------+--------------------+------------+-------------+---------------+----------+----------------+-----------------+---------------+--------------------+--------------+-----------------------+----+\n",
      "|      id|diagnosis|radius_mean|texture_mean|perimeter_mean|area_mean|smoothness_mean|compactness_mean|concavity_mean|concave points_mean|symmetry_mean|fractal_dimension_mean|radius_se|texture_se|perimeter_se|area_se|smoothness_se|compactness_se|concavity_se|concave points_se|symmetry_se|fractal_dimension_se|radius_worst|texture_worst|perimeter_worst|area_worst|smoothness_worst|compactness_worst|concavity_worst|concave points_worst|symmetry_worst|fractal_dimension_worst|_c32|\n",
      "+--------+---------+-----------+------------+--------------+---------+---------------+----------------+--------------+-------------------+-------------+----------------------+---------+----------+------------+-------+-------------+--------------+------------+-----------------+-----------+--------------------+------------+-------------+---------------+----------+----------------+-----------------+---------------+--------------------+--------------+-----------------------+----+\n",
      "|  842302|        M|      17.99|       10.38|         122.8|     1001|         0.1184|          0.2776|        0.3001|             0.1471|       0.2419|               0.07871|    1.095|    0.9053|       8.589|  153.4|     0.006399|       0.04904|     0.05373|          0.01587|    0.03003|            0.006193|       25.38|        17.33|          184.6|      2019|          0.1622|           0.6656|         0.7119|              0.2654|        0.4601|                 0.1189|null|\n",
      "|  842517|        M|      20.57|       17.77|         132.9|     1326|        0.08474|         0.07864|        0.0869|            0.07017|       0.1812|               0.05667|   0.5435|    0.7339|       3.398|  74.08|     0.005225|       0.01308|      0.0186|           0.0134|    0.01389|            0.003532|       24.99|        23.41|          158.8|      1956|          0.1238|           0.1866|         0.2416|               0.186|         0.275|                0.08902|null|\n",
      "|84300903|        M|      19.69|       21.25|           130|     1203|         0.1096|          0.1599|        0.1974|             0.1279|       0.2069|               0.05999|   0.7456|    0.7869|       4.585|  94.03|      0.00615|       0.04006|     0.03832|          0.02058|     0.0225|            0.004571|       23.57|        25.53|          152.5|      1709|          0.1444|           0.4245|         0.4504|               0.243|        0.3613|                0.08758|null|\n",
      "|84348301|        M|      11.42|       20.38|         77.58|    386.1|         0.1425|          0.2839|        0.2414|             0.1052|       0.2597|               0.09744|   0.4956|     1.156|       3.445|  27.23|      0.00911|       0.07458|     0.05661|          0.01867|    0.05963|            0.009208|       14.91|         26.5|          98.87|     567.7|          0.2098|           0.8663|         0.6869|              0.2575|        0.6638|                  0.173|null|\n",
      "|84358402|        M|      20.29|       14.34|         135.1|     1297|         0.1003|          0.1328|         0.198|             0.1043|       0.1809|               0.05883|   0.7572|    0.7813|       5.438|  94.44|      0.01149|       0.02461|     0.05688|          0.01885|    0.01756|            0.005115|       22.54|        16.67|          152.2|      1575|          0.1374|            0.205|            0.4|              0.1625|        0.2364|                0.07678|null|\n",
      "|  843786|        M|      12.45|        15.7|         82.57|    477.1|         0.1278|            0.17|        0.1578|            0.08089|       0.2087|               0.07613|   0.3345|    0.8902|       2.217|  27.19|      0.00751|       0.03345|     0.03672|          0.01137|    0.02165|            0.005082|       15.47|        23.75|          103.4|     741.6|          0.1791|           0.5249|         0.5355|              0.1741|        0.3985|                 0.1244|null|\n",
      "|  844359|        M|      18.25|       19.98|         119.6|     1040|        0.09463|           0.109|        0.1127|              0.074|       0.1794|               0.05742|   0.4467|    0.7732|        3.18|  53.91|     0.004314|       0.01382|     0.02254|          0.01039|    0.01369|            0.002179|       22.88|        27.66|          153.2|      1606|          0.1442|           0.2576|         0.3784|              0.1932|        0.3063|                0.08368|null|\n",
      "|84458202|        M|      13.71|       20.83|          90.2|    577.9|         0.1189|          0.1645|       0.09366|            0.05985|       0.2196|               0.07451|   0.5835|     1.377|       3.856|  50.96|     0.008805|       0.03029|     0.02488|          0.01448|    0.01486|            0.005412|       17.06|        28.14|          110.6|       897|          0.1654|           0.3682|         0.2678|              0.1556|        0.3196|                 0.1151|null|\n",
      "|  844981|        M|         13|       21.82|          87.5|    519.8|         0.1273|          0.1932|        0.1859|            0.09353|        0.235|               0.07389|   0.3063|     1.002|       2.406|  24.32|     0.005731|       0.03502|     0.03553|          0.01226|    0.02143|            0.003749|       15.49|        30.73|          106.2|     739.3|          0.1703|           0.5401|          0.539|               0.206|        0.4378|                 0.1072|null|\n",
      "|84501001|        M|      12.46|       24.04|         83.97|    475.9|         0.1186|          0.2396|        0.2273|            0.08543|        0.203|               0.08243|   0.2976|     1.599|       2.039|  23.94|     0.007149|       0.07217|     0.07743|          0.01432|    0.01789|             0.01008|       15.09|        40.68|          97.65|     711.4|          0.1853|            1.058|          1.105|               0.221|        0.4366|                 0.2075|null|\n",
      "|  845636|        M|      16.02|       23.24|         102.7|    797.8|        0.08206|         0.06669|       0.03299|            0.03323|       0.1528|               0.05697|   0.3795|     1.187|       2.466|  40.51|     0.004029|      0.009269|     0.01101|         0.007591|     0.0146|            0.003042|       19.19|        33.88|          123.8|      1150|          0.1181|           0.1551|         0.1459|             0.09975|        0.2948|                0.08452|null|\n",
      "|84610002|        M|      15.78|       17.89|         103.6|      781|         0.0971|          0.1292|       0.09954|            0.06606|       0.1842|               0.06082|   0.5058|    0.9849|       3.564|  54.16|     0.005771|       0.04061|     0.02791|          0.01282|    0.02008|            0.004144|       20.42|        27.28|          136.5|      1299|          0.1396|           0.5609|         0.3965|               0.181|        0.3792|                 0.1048|null|\n",
      "|  846226|        M|      19.17|        24.8|         132.4|     1123|         0.0974|          0.2458|        0.2065|             0.1118|       0.2397|                 0.078|   0.9555|     3.568|       11.07|  116.2|     0.003139|       0.08297|      0.0889|           0.0409|    0.04484|             0.01284|       20.96|        29.94|          151.7|      1332|          0.1037|           0.3903|         0.3639|              0.1767|        0.3176|                 0.1023|null|\n",
      "|  846381|        M|      15.85|       23.95|         103.7|    782.7|        0.08401|          0.1002|       0.09938|            0.05364|       0.1847|               0.05338|   0.4033|     1.078|       2.903|  36.58|     0.009769|       0.03126|     0.05051|          0.01992|    0.02981|            0.003002|       16.84|        27.66|            112|     876.5|          0.1131|           0.1924|         0.2322|              0.1119|        0.2809|                0.06287|null|\n",
      "|84667401|        M|      13.73|       22.61|          93.6|    578.3|         0.1131|          0.2293|        0.2128|            0.08025|       0.2069|               0.07682|   0.2121|     1.169|       2.061|  19.21|     0.006429|       0.05936|     0.05501|          0.01628|    0.01961|            0.008093|       15.03|        32.01|          108.8|     697.7|          0.1651|           0.7725|         0.6943|              0.2208|        0.3596|                 0.1431|null|\n",
      "|84799002|        M|      14.54|       27.54|         96.73|    658.8|         0.1139|          0.1595|        0.1639|            0.07364|       0.2303|               0.07077|     0.37|     1.033|       2.879|  32.55|     0.005607|        0.0424|     0.04741|           0.0109|    0.01857|            0.005466|       17.46|        37.13|          124.1|     943.2|          0.1678|           0.6577|         0.7026|              0.1712|        0.4218|                 0.1341|null|\n",
      "|  848406|        M|      14.68|       20.13|         94.74|    684.5|        0.09867|           0.072|       0.07395|            0.05259|       0.1586|               0.05922|   0.4727|      1.24|       3.195|   45.4|     0.005718|       0.01162|     0.01998|          0.01109|     0.0141|            0.002085|       19.07|        30.88|          123.4|      1138|          0.1464|           0.1871|         0.2914|              0.1609|        0.3029|                0.08216|null|\n",
      "|84862001|        M|      16.13|       20.68|         108.1|    798.8|          0.117|          0.2022|        0.1722|             0.1028|       0.2164|               0.07356|   0.5692|     1.073|       3.854|  54.18|     0.007026|       0.02501|     0.03188|          0.01297|    0.01689|            0.004142|       20.96|        31.48|          136.8|      1315|          0.1789|           0.4233|         0.4784|              0.2073|        0.3706|                 0.1142|null|\n",
      "|  849014|        M|      19.81|       22.15|           130|     1260|        0.09831|          0.1027|        0.1479|            0.09498|       0.1582|               0.05395|   0.7582|     1.017|       5.865|  112.4|     0.006494|       0.01893|     0.03391|          0.01521|    0.01356|            0.001997|       27.32|        30.88|          186.8|      2398|          0.1512|            0.315|         0.5372|              0.2388|        0.2768|                0.07615|null|\n",
      "| 8510426|        B|      13.54|       14.36|         87.46|    566.3|        0.09779|         0.08129|       0.06664|            0.04781|       0.1885|               0.05766|   0.2699|    0.7886|       2.058|  23.56|     0.008462|        0.0146|     0.02387|          0.01315|     0.0198|              0.0023|       15.11|        19.26|           99.7|     711.2|           0.144|           0.1773|          0.239|              0.1288|        0.2977|                0.07259|null|\n",
      "+--------+---------+-----------+------------+--------------+---------+---------------+----------------+--------------+-------------------+-------------+----------------------+---------+----------+------------+-------+-------------+--------------+------------+-----------------+-----------+--------------------+------------+-------------+---------------+----------+----------------+-----------------+---------------+--------------------+--------------+-----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import csv of spam and ham (not spam)\n",
    "start_data = spark.read.format(\"csv\").\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    load(\"data.csv\")\n",
    "start_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.6267605633802817\n",
      "Testing Data Score: 0.6223776223776224\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {model.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
